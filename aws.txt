

    SECTION 8  (continue )
    LEARNING AWS
Iam : Permissions : 

- Users or groups can be assigned JSON documents called policies
- These policlies define the permissions of the users 
- In AWS you apply the least privilege principle  : donâ€™t give more
Permissions than a user needs

IAM Policies Structure
Consists of 
-	Version : policy language version 
-	Id : an identifier for the policy
-	Statement : one or more individual statements (required)
Statements consists of 
-	Sid : an identifier for the statement (optional)
-	Effect : whether the statement allows or denies access ( Allow , Deny)
-	Principal : account/user/role to which this policy applied to
-	Action : list of actions this policy allows or denies
-	Resource :list of resources to which the actions applied to
-	Condition : conditions for when this policy is in effect (optional)

IAM â€“ PASSWORD POLICY
2 Defense mechanisms : 
-	Strong passwords = higher security for your account 
-	In AWS , you can setup a password policy

o	Set a minium password length
o	Require specific character types
ï‚§	Including uppercase letters 
ï‚§	Lowercase letters
ï‚§	Number , or non alphanumeric character (ki tu dac biet )
o	Allow all IAM users to change their own passwords
o	Require user to change their password after some time (password exp)
o	Prevent password re-use
Multi Factor Authen â€“ MFA
-	Users have acess to your account and can possibly change config or delete
Resources in your AWS account 
-	You want to protect your Root Accounts and IAM users
-	MFA = password you know + security device you own

Section 8
â€¢	Application load balancer (v2)
â€¢	Application load balancers is Layer 7 (HTTP)
â€¢	Load balancing to multiple HTTP app (target groups)
â€¢	Load balancing to multiple applications on the same machine 
â€¢	Support for http/2 and websocket
â€¢	Support redirects (http/https)
â€¢	Routing tables to different target groups:
o	Routing path url
o	Routing hostname url
o	Routing on query string, header 
â€¢	ALB are a great fit for micro and container app
â€¢	Has a port mapping feature to redirect to dynamic port in ecs
â€¢	In comparsion , weâ€™d need multiple classic load balancer per application 
â€ƒ
Application load balancer (v2) target group

â€¢	EC2 instances (can be managed by an auto scaling group) â€“ HTTP
â€¢	ECS task (managed by ecs it self) -HTTP
â€¢	Lambda function â€“ HTTP req/ JSON event
â€¢	Private IP address
â€¢	ALB can route to multiple target group
â€¢	Health checks are at the target group level

ALB (v2) Good to know
â€¢	Fixed hostname 
â€¢	the application servers donâ€™t see the IP client 
ïƒ°	inserted in the header X-forwarded-for
Network Load Balancer (ELB v2)
â€¢	Network load balancers (Layer 4 ) allow to
o	Forward TCP & UDP traffic to your instances
o	Handle millions of request per seconds 
o	Less latency ~100ms (vs 400 ms for ALB)
â€¢	NLB as one static IP per AZ, and supports assigning Elastic IP
â€¢	NLB as used for extreme performance , TCP or UDP traffic
â€¢	Not free 

Gateway Load Balancer
â€¢	Deploy scale , and manage a fleet of 3rd party network
â€¢	Operates Layer 3 ( network layer ) â€“ IP packets 
â€¢	Combines the following function : 
o	Transparent network gateway
o	Load balancer
â€¢	Uses the GENEVE port 6081
â€¢	Ec2 instance
â€¢	Private IP address
Sticky Sessions (Session Affinity)
â€¢	This works for Classic Load balancers & application Load balancer
â€¢	The â€œcookieâ€ used for stickiness has an exp date
â€¢	Enabling stickness may bring imbalance to the load over the be EC2
Sticky Sessions â€“ Cookie Names 
â€¢	Application-based Cookie
o	Custom cookie
ï‚§	Donâ€™t use AWSALB , AWSALPAPP , AWSALBTG
o	Application cookie
ï‚§	As AWSALBAPP
â€¢	Duration-based cookie
o	Cookie generated by the load balancer
o	Cookie name is AWSALB for alb , AWSELB

Cross-Zone Load balancing
â€¢	Application Load Balancer
o	Always on 
o	No changes for inter AZ
â€¢	Network Load Balancer
o	Default as disabled
o	Pay charges 
â€¢	Classic Load Balancer
o	Disabled by default
o	No charges for inter AZ





Section 9 : 
SSL - Basics 
-   An ssl Certificate allows traffic between your clients and your load balancer
to be encrypted in transit 
-   SSL refer to Secure Sockets Layer , used to Encrypt connections 
-   TLS refers to transport Layer security , which is a newer version
-   Nowadays , TLS cert are mainly used , but SSL refer
-   Public ssl cert ase issues by (CA)
-   SSL cert have an exp ( you set ) and must be renewed
Load Balancer - SSL Cert 
-   The load Balancer uses an X509 cert (SSL/TLS)
-   You can manage cert usign ACM
-   You can create upload your own cert alternatively
    HTTPS Listener : 
    +   You must specify a default cert
    +   add an option of cert when support multi domain
    +   Clients can use SNI(Server Name indi) to specify hostname
    +   ability to specify a security policy to support SSL/TLS

SSL -  Server Name Indication
-   SNI solves the problem of loading multi SSL cert onto one web Server
-   It's a "newer" protocol, and requies the client to indicate the hostname
    of the target server in the initial SSL 
-   The server will then find the correct cert , of return the default one
    Note : 
        +   Only work for ALB-NLB
        +   Does not work for CLB
ELB - SSL cert
-   Classic Load Balancer (v1)
    +   support only one SSL cert
    +   Must use multi CLB for multi host with multi cert
-   Application Load Balancer (v2)
    +   Support multi Listener with multi SSL cert
    +   Uses SNI to make it work
-   Network Load Balancer (v2)
    +   Support multi Listener with multi cert 
    +   Users SNI

Connection Draining
-   Feature naming
    +   Connection Draining : CLB
    +   Deregistration : ALB - NLB
-   Time to complete "in flight request " while the instance is 
    de-registering or unhealthy
-   Stops sending new request to ec2 instance which ism  de-registering
-   Between 1 to 3600 sec (default as 300 , 0 as disabled)
-   Set to a low value if your request are short 
Auto scalling Group? 
-   The goal of Auto Scalling Group(ASG) is to :
    +   Scale out (add EC2 instance) to match an increased Load
    +   Scale in (remove EC2 instance) to match a decrease load 
    +   Ensure we have a minium and a maximum number of ec2 instances running
    +   Auto matically register new instances to a load balancer 
    +   Re-create an EC2 instance in case a previous one is terminated
-   ASG are free ( but as pay for the underlying EC2 )
Auto Scalling Group Attr
-   A Lunch Template 
    +   AMI  and Instance Type 
    +   EC2 User Data
    +   EBS volumes
    +   Security Groups
    +   SSH key pair 
    +   IAM roles for Your EC2 instances
    +   Network + Subnet Infor 
    +   Load Balancer infor 
-   Minsize / Maxsize / Intial Capacity
-   Scaling Policies
Auto Scalling - CloudWatch Alarams and Scalling 
-   It is possible to scale an ASG based on CloudWatch alarms 
-   An Alarm monitors a metric (such as Average CPU , or a custom metric)
-   Metrics such as Average CPU are computed for the overall ASG instances
-   Based on the alarm : 
    +   We can create scale-out policies (increase  number instance)
    +   We can create scale- policies (decrease  number instance)

ASG - Dynamic Scalling Policies
-   Target Tracking Scaling
-   Simple/Step Scalling 
    +   When a Cloudwatch alarm is trigger ( >  : add unit )
    +   When a Cloudwatch alarm is trigger ( <  : remove)
-   Scheduled Application  
    +   Anticipate a scaling based on known usage pattern 

ASG - Predictive Scalling 
    +   Predictive scalling : continuously forecast load and schedule scaling ahead

ASG - Good metrics 
    -   CPU Utilization : Average CPU utilization across your instances 
    -   RequestCountPerTarget : to make sure the number of requests per EC2 instances is stable 
    -   Average Network I/O 
    -   Any custom metric (using Cloudwatch)
ASG - Scalling Coldowns
-   After a scaling activity happens, you are in the 
    cooldown period (default 300s )
-   During the coldonw period, the ASG will not 
    launch or terminate additional instances
-   Advice : Use a ready-to-use API to reduce config time in 
    order to be serving req faster and reduce the cooldown period

ASG for SA
-   ASG default Termination Policy 
    1 : find the az which has the most number of instance
    2 : if there are multi instance in the az to choose from,
    delete the one with the oldest launch config
-   ASG tries the balance the number of instances across AZ by default 

ASG for SA - Lifecycle hook 
-   By default as soon as an instance is launched in an ASG it's in service
-   You have the ability to perform extra steps before instance goes in service
-   you have the ability to perform some actions before instance is terminated

ASG for SA - Launch Template vs Launch configurations
-   Both:
    +   ID of the Amazon Machine Image (AMI) , the intsance type , a key pair 
    security groups and the other param(e.g : ec2 user-data) that you
    use to launch EC2 instance 
-   Launch Config (legacy)
    +   Must be re-created every time 
-   Launch Template (newer)
    +   Can have multiple versions
    +   Create pram subsets
    +   Provision using both On-Demand and Spot instances
    +   Can use T2 unlimited burst Feature
    +   Recommended by AWS going forward

Section 9 : 
AWS RDS Overview
-   RDS stands for Relational Database service
-   it's a managed DB service for DB use SQL as a query language
-   It allows you to create databases in the cloud that are managed by AWS
    +   Postgres
    +   MySQL
    +   MariaDB
    +   Oracle
    +   Microsoft SQL Server
    +   Aurora (not critial remember it )

Advantage over using RDS versus deploying DB on ec2
-   RDS us a managed service:
    +   Automated provisioning , OS patching 
    +   Continuous backups and restore to specific timestamp
    +   Monitoring dashboards 
    +   Read replicas for improved read performance
    +   Multi AZ setup for DR (Disaster Recovery)
    +   Scaling capability (vertical and horizontal)
    +   Storage backed by EBS (gp2 or io1)
-   BUT you CAN't SSH into your instances

RDS backups
-   Backups are automatically enabled in RDS
-   Automated backups: 
    +   Daily full backup of the database 
    +   Transaction logs are backed-up by RDS every 5 minutes
    => ability to restore to any point in time (5 min)
    +   7 days retention (can be increased to 35 days )
-   DB Snapshots:
    +   Manually triggered by the user
    +   Retention of backup for as long as you want

RDS -  Storage Auto Scalling 
-   Helps you increase Storage on your RDS DB instance dynamically
-   When RDS detects you are running out of free database storage, it scales auto
-   Avoid manually scaling your database storage
-   you have to set MAXIMUM STORAGE THRESHOLD 
-   Automatically modify storage if :
    +   Free storage is less than 10% of allocated storage
    +   Low-storage lasts at least 5 min
    +   6 hours have passed since last modification
-   Useful for appications with unpredictable workloads
-   Supports all RDS database engines

RDS Read Replicas for read scalability
-   UP TO 5 READ REPLICAS
-   With in AZ, Cross Az or Cross Region
-   Replication is ASYNC so read are eventtually consistent
-   Replicas can be promoted to their own DB

RDS Read Replicas - Use Cases 
-   You have o production database that is taking on normal load
-   You want to run a reporting application to run some analytics
-   You create a read replica to run the new workload there
-   The production application is unaffected
-   Read replicas are used for SELECT (=read) only kind of statements
    (not INSERT, UPDATE , DELETE)

RDS - Network Cost

-   same AZ with other region : free (vd : us-east-1a <> us-east-1b)
-   other AZ same region : not free (vd : us-east-1a <> eu-west-1b)

RDS Multi AZ (Disaster Recovery)
-   SYNC Replication
-   One DNS name - automatic app failover to standby
-   Increase availability
-   Failover in case of loss of AZ, loss of network, instance or storage failure
-   No manual intervention in apps
-   Not used for scaling
+   Note : The Read Replicas be setup as MULTI AZ for Disaster Recovery

RDS - FROM Single-AZ to multi AZ
-   Zero downtime operation
-   Just click on "modify" for the database config
-   The following happens internally
    +   A snapshot is taken
    +   A new DB is restored from the snapshot in a new AZ 
    +   Synchonization is established between the two databases 

RDS Security - Encryption
-   At rest encryption 
    +   Possibility to encrypt the master and replicas with AWS-AES 256
    +   Encryption has to be defined at lunch time
    +   If the master is not encrypted , the read replicas cannot be encrypted
    +   Transparent Data Encryption (TDE) available for Oracle and SQL Server
-   In Flight encryption
    +   SSL cert options with trust cert when connecting to database
    +   Provide SSL options with trust cert when connecting to database 
    +   To enforce SSL:
        *   PostgreSQL : _ssl = l in the AWS RDS Console
        *   My SQL : with in the DB  .... REQUIRE SSL
RDS Encryption operations 
-   Encrypting RDS Backups
    +   Snapshots of un-encrypted RDS databases are un-encrypted
    +   Snapshots of encrypted RDS databases are encrypted
    +   Can copy a snapshot into an encrypted one 
-   To encrypt an un-encrypted RDS database 
    +   Create a snapshot of the un-encrypted database
    +   Copy the snapshot and enable encryption for the snapshot
    +   Restore the database from the encrypted snapshot
    +   Migrate applications to the new database , and delete the old database

RDS Security - Network & IAM
-   Network Security
    +   RDS databases are usually deployed within a private subnet, not a public one
    +   RDS security work by leveraging security groups- it controls
    with IP/Security group can COMMUNICATE with RDS 
-   Access Management 
    +   IAM policies help control who can manage AWS RDS
    +   Traditional Username and Password can be used to login into the database 
    +   IAM-based authen can be used to login into RDS MySQL  & PostgreSQL

RDS - IAM authen
-   IAM database authen work with MYSQL AND POSTGRESQL
-   You don't need a password, just an authen token obtained thorugh IAM - RDS API
-   Auth token expired when after 15min
-   Benefits : 
    +   Network in/out must be encrypted using SSL
    +   IAM to centrally manage users instead of DB 
    +   Can Ieverage IAM Roles and EC2 Instance profiles for easy integration 

RDS Security - Summary 
-   Encryption at rest
    +   Is done only when you first create the DB instance
    +   or : unencrypted DB => snapshot => copy snapshot as encrypted
        => create DB from snapshot 
-   Your responsibility
    +   Check the ports / IP / Security group inbound rules in DB's SG
    +   In-database user creation and permissions or manage through IAM
    +   Creating a database with or without public acess 
    +   Ensure param group or DB is configured to only allow SSL 
-   AWS responsibility
    +   No SSH access 
    +   No manual DB patching 
    +   No manucal OS patching 
    +   No way to audit the underlying instance

RDS Security - encryption
-   At rest encryption
    +   Possibility to encrypt the master & the read replicas with AWS KMS - AES - 256 encryption
    +   Encryption has to be defined at lunch time 
    +   If the master is not encrypted , the read replicas CANNOT be encrypted
    +   Transparent Data Encryption(TDE) available for Oracle and SQL Server
-   In-flight encryption
    +   SSL cert to encrypt data to RDS in flight 
    +   Provide SSL options with trust cert when connecting to database 
    +   To enforce SSL : 
        *   PostgreSQL : as Params Group ( rds.force_ssl  = 1 )
        *   MySQL : with in the DB query  ( ...... + REQUIRE SSL)

RDS Encryption operation
-   Encrypting RDS backup
    +   Snapshots of un-encrypted RDS database are un-encrypted
    +   snapshot of encrypted RDS database are encrypted
    +   Can copy a snapshot into an encrypted one
-   To encrypt an un-encrypted RDS database 
    +   Create a snapshot of the un-encrypted database
    +   Copy a snapshot and enable encryption for the snapshot
    +   Restore the database from the encrypted snapshot
    +   Migrate application to the new database, and delete the old database

RDS Security - Network & IAM
-   Network security
    +   RDS database are usually deployed within private subnet,not a public one
    +   RDS security work by leveraging SECURITY GROUPS - it controls which IP /
        Security group can COMMUNICATE with RDS
-   Access Management
    +   IAM policies help control who can MANAGE AWS RDS (through the RDS API)
    +   Traditional Username and Password can be used to LOGIN INTO the database
    +   IAM-based authen can be used to login into RDS MYSQL & PostgreSQL

RDS - IAM Authen
-   IAM database authen work MYSQL - PostgreSQL
-   You don't need a password,just an authen token obtained through IAM-RDS called
-   Auth token expried than 15min
-   Benefits:
    +   Network i/o must be encrypt using SSL
    +   IAM to centrally manage users instead of DB
    +   Can leverage IAM ROles and EC2 Instance profiles for easy integration


RDS Security - Summary 
-   Encryption at rest : 
    +   Is done only when you first create the DB Instance
    +   Or : unencrypted DB => snapshot => copy snapshot as encrypted
        => Create DB from snapshot
-   Your responsibility
    +   Check the ports/ IP / security group inbound rules in DB's SG
    +   In-database user creation and permissions or manage through IAM
    +   Creating a database with or without public access
    +   Ensure param group or DB is config to only allow SSL connection
-   AWS responsibility
    +   No SSH access
    +   No manual DB patching 
    +   No manual OS patching
    +   No way to audit the underlying instance

Amazon Aurora 
-   is proprietary from AWS (not open source)
-   Postgres and MYSQL are both supported as Aurora DB
-   Aurora is "AWS cloud optimized" and claims 5x perform improvement over MYSQL
    on RDS , over 3x the performance of Postgres on RDS
-   Aurora storage automatically grows in increments of 10gb , upto 128gb
-   Aurora can have 15 replicas while MYSQL as 5,and the replication process faster
-   Failover in Aurora is instantaneous. It's HA native
-   Aurora costs more than 20% RDS - but is more efficient 

Aurora high availability and read scaling 
-   6 copies of your data across 3az
    +   3 copies out of 6 need for read
    +   self healing with peer-to-peer replication
    +   Storage is striped across 100s of volumes
-   One Aurora instance takes writes ( master )
-   Automated failover for master in less than 30s 
-   master + up to 15 Aurora Read Replicas serve needs
-   Support for cross region replication

Feature of Aurora
-   Automatic failover
-   Backups and Recovery
-   Isolation and security
-   Industry compliance
-   Push - button scaling 
-   Automated Patching with Zero Downtime
-   Advanced Monitoring 
-   Routine Maintenance
-   Backtrack: restore data at any point of time without using backup

Aurora security
-   Similar to RDS b.c uses the same engines
-   Encryption at rest using KMS
-   Automated backups , snapshots and replicas are also encrypted
-   Encryption in flight using SSL 
-   POSSIBILITY TO AUTHEN USING IAM TOKEN 
-   You are responsible for protecting the instance with security groups 
-   you can't SSH

Aurora Serverless 
-   Automated database instantiation and auto - scaling based on actual usage
-   good for infrequent , intermittent or unpredictable workload
-   no capability planning need
-   pay per seconds , can be more cost effect
Global Aurora 
-   Aurora Cross Region read replicas 
    +   Useful for disaster Recovery
    +   Simple to put in place
-   Aurora Global database (Recommended)
    +   I primary Region (read/write)
    +   Up to 5s (read-only) regions , replication lag is less than I second
    +   Up to 15 Read replica per second region
    +   Helps for decrasing latency
    +   Promoting another region  has an RTO of < I minutes

Aurora ML 
-   Enables you to add ML-based prediction to your application via SQL
-   Simple , optimized, and secure integration between Aurora and AWS ML services
-   Supported services 
    +   Amazon Sage maker
    +   Amazon comprehend
-   You don't need to have ML experience 
-   Use cases : fraud detection , ads targeting , sentiment analysis
    product Recommendedations 

Amazon ElastiCache Overview
-   The same way RDS is to get managed Relational Database...
-   ElastiCache is to get managed Redis or Memcached 
-   Caches are in-memory databases with really high performance , low latency
-   Helps reduce load off of databases for read intensive workloads
-   Helps make your application stateless
-   AWS takes care of OS Maintenance / patching , optimizations , setup, config
    Monitoring , failure recovery and backups 
-   USING ELASTICACHE INVOLVES HEAVY APP CODE CHANGES 

ElastiCache SA - DB Cache 
-   Application qurey ElastiCache, if not available
    get from RDS and store in ElastiCache 
-   Helps relieve load in RDS
-   Cache must be an invalidation strategy to make sure only the most
    current data is used in there 

ElastiCache SA - User Session store
-   User logs into any of the application
-   The application writes the session data into ElastiCache
-   The user hits another instance of our application
-   The instance retrieves the data and the user is already logged in 

ElastiCache -  Redis vs Memcached
Redis : 
-   Multi AZ with Auto-failover 
-   Read Replicas to scale read and have high availability
-   Data Durability using AOF persistence
-   BACKUP AND RESTORE FEATURE 
Memcached : 
-   Multi-node for partitioning of data (sharding )
-   No high availability (replication)
-   Non persistent
-   No backup and restore 
-   Multi  threaded achitecture 

ElastiCache - Cache Security
-   All caches in ElastiCache
    +   DO NOT SUPPORT IAM AUTHEN
    +   Iam policies on ElastiCache are only useds for AWS API-level security
-   REDIS AUTH 
    +   You can set a "password/token" when you create a redis cluster
    +   This is an extra level of security for your cache 
    +   Support SSL in flight encryption
-   Memcached
    +   Supports SASL-based authen

Patterns for ElastiCache
-   LAZY LOADING  
-   WRITE THROUGH
-   SESSION STORAGE "
-   Quote:There are only two hard things in Computer Science:cache invalid & naming

ElastiCache - Redis UseCase 
-   Gaming Leaderboards are computationally complex
-   Redis Sorted sets gurantee both uniqueness and element ordering 
-   Each time a new element added, it's ranked in realtime, then add in correct order

Note for session 9 : LIST PORT Familiar
-   You should be able to differentitate between an important HTTPS - 443
    and Database port ( PostgreSQL  - 5432 )
-   IMPORTANT PORTS : 
    +   FTP : 21
    +   SSH : 22
    +   SFTP : 22 (same as SSH)
    +   HTTP : 80
    +   HTTPS : 443
    RDS DATABASE PORTS : 
    +   PostgreSQL : 5432
    +   MySQL : 3306
    +   Oracle RDS : 1521
    +   MariaDB : 3306
    +   Aurora: MySQL + PostgreSQL


Section 10 : Route 53
What is DNS
-   Domain name system which translates the human friendly hostnames into The
    machine IP address
-   www.google.com => 172.217.18.36
-   DNS is the backbone of the internet
-   DNS uses hierarchial naming structure 

DNS Terminologies
-   DOMAIN REGISTRAR 
-   DNS RECORDS
-   ZONE FILE 
-   NAME SERVER
-   TOP LEVEL DOMAIN (TLD)
-   SECOND LEVEL DOMAIN

Amazon Route 53
-   A highly available, scalable , fully managed and authortative DNS 
    +   Authortative  =the customer (you) can update the DNS records 
-   Route 53 is also a Domain REGISTRAR
-   Ability to check the health of your resuorces 
-   The only AWS service which provides 100%
-   Why route 53 ? 53 is a reference to the traditional DNS port 

Route 53 - Records 
-   Each record contains : 
    +   DOMAIN / SUBDOMAIN NAME 
    +   Record Type 
    +   Value 
    +   Routing policy
    +   TTL
-   Route 53 supports the following DNS record types 
    +   (must know ) A / AAAA / CNAME / NS 

Route 53 - Record Types 
-   A : maps a hostname to IPv4
-   AAAA : maps a hostname to IPv6
-   CNAME : maps a hostname to another hostname 
    +   The target is a domain name which must have an A or AAAA record 
    +   Can't crate a CNAME record for the top node of a DNS namespace (Zone Apex)
-   NS - Name Servers for the Hosted Zone 
    +   Control how traffic is routed for a domain  

Route 53 - Hosted Zones 
-   A container for records that define how to route traffic to a domain and sub it
    +   PUBLIC HOSTED ZONES 
    +   PRIVATE HOSTED ZONES
-   You pay $0.5/month with 1 hosted zone 

Route 53 - Records TTL (Time to Live)
-   High TTL - vd 24hr
    +   Less trafic on route 53
-   Low TTL - vd 60sec
    +   More traffic on route 53
    +   Records are outdated for less time 
    +   Easy to change records
-   EXCEPT FOR ALIAS RECORDS , TTL IS MANDATORY FOR EACH DNS RECORDS 
 
CNAME vs ALIAS
-   AWS Resources (Load Balancer , CloudFront...) expose an aws hostname :
-   CName : 
    +   Points a hostname to any other hostname
    +   ONLY FOR NON ROOT DOMAIN 
-   Alias : 
    +   Points a hostname to an AWS resource 
    +   Works for ROOT DOMAIN and NON ROOT DOMAIN
    +   Free of charge
    +   Native health check

Route 53 - Alias Records Targets 
-   Elastic Load Balancer
-   cloudFront Distribution
-   Api Gateway
-   Elastic Beanstalk environments
-   S3 Websites
-   VPC Interface Enpoints
-   Global Accelerator
-   Route 53 records in the same hosted zone 
-   YOU CANNOT SET AN ALLIAS RECORDS FOR AN EC2 DNS name 

Route 53 - Routing policies 
-   Define how Route 53 responds to DNS queries 
-   Don't get confused by the word "Routing "
    +   It's not the same as Load balancer routing which routes the traffic 
    +   DNS does not route any traffic , it only responds to the DNS queries
-   Route 53 Supports the following Routing policies 
    +   Simple 
    +   Weighted 
    +   Failover
    +   Latency based
    +   Geolocation
    +   Multi-value Answer
    +   Geoproximity (using Route 53 Traffic Flow feature)

Routing Policies - Simple
+   Typlically, route traffic to a single resource
+   Can specify multiple values in the same record
+   If multiple values values are returned, a random one is chosen by the client
+   When alias enabled , specify Only One AWS resource
+   Can't be associated with health checks

Routing Policies - Weighted
+   Control the % of the request that go to each specific resource
+   Assign each record a relative weight :
    Traffic (%) = Weight for a record / sum all weight for all records 
+   DNS Records MUST HAVE SAME NAME - TYPE
+   Can be associated with Health checks
+   Use cases : Load balancing between regions , testing new application versions
+   Assign a weight of 0 to a record to stop sending traffic to a resource
+   If all records have weight of 0, then all records will be returned equally 

Routing policies - Latency - based 
+   Redirect to the resource that has the least latency close to us 
+   Super helpful when latency for users is a priority
+   Latency is based on traffic between users and AWS Regions 
+   Germany users maybe directed to the US (if that's the lowest latency )
+   Can be associated with Health Checks (has a failover capability)

Route 53 - Health Checks 
-   HTTP health checks ONLY PUBLIC RESOURCES
-   Health Check => Automated DNS Failover
    1 : Health checks that monitors an enpoint 
    2 : Health checks that montior other health checks
    3 : Health checks that monitor Cloudwatch Alarms 
-   Health Checks are intergated with CW metrics

Health checks - Montior an Endpoint 
-   About 15 global health checker will check the enpoint health
    +   Healthy / unhealthy Threshold 
    +   Interval - 30s
    +   Supported protocol: HTTP,HTTPS and TCP 
    +   If > 18% of health checkers report the enpoint is healthy , Route 53
        considers it Healthy. Otherwise , it's unhealthy
    +   Ability to choose which locations you want Route 53 to use
-   Health Checks pass only when the endpoint responds with the 2xx and 3xx status
-   Health checks  pass/fail based on the text in the first 5120b / response
-   Config you router/firewall to allow incoming req from Route 53 health checks

Route 53 - Calculated Health checks
-   Combines the results of multiple Health checks into a single health check
-   You can use OR , AND , or NOT 
-   Can monitor up to 256 Child Health Checks
-   Specify how many of the health checks need to pass to make the parent pass
-   Usage:perform maintenance your website without causing all health check to fail

Health checks - Private hosted zones 
-   Route 53 health checkers are outside the VPC
-   They can't access private endpoints
-   You can create a CloudWatch METRIC and associated a CloudWatch ALARM,
    Then create a Health check that check the alarm itself

Route Policies - Geolocation
-   different from Latency-based!
-   THIS ROUTING IS BASED ON USER LOCATION 
-   Specify location by Continent , Country or by US State 
-   Should create a Default record 
-   Use cases: website localization, restrict content distribution, load balancing
-   Can be associated with Health Check

Geoproximity Routing Policy
-   Route traffic to your resources based on the geographic location of user
-   Ability TO SHIFT MORE TRAFFIC TO RESOURCES BASED on the defined BIAS
-   To change the size of the geographic region , specify BIAS values: 
    +   To expand (1 to 99) - more traffic to the resource
    +   To shrink (-1 to 99) - less traffic to the resource
-   Resources can be :
    +   AWS resources 
    +   Non-AWS Resources
-   You MUST USE ROUTE 53 TRAFFIC FLOW to use THIS FEATURE

Routing policies - Multi value 
-   Use when routing traffic to multiple resources 
-   Route 53 return multiple values / resources 
-   Can be associated with health checks 
-   Up to 8 healthy records are returned for each multi-value query
-   Multi value is not a substitue for having an ELB 

Domain Registar vs DNS service
-   You buy or register your domain name with a Domain Registar typlically by 
    paying annual charges (domain company )
-   The domain Registar provides you with a DNS service to manage your DNS record
-   But u can use another DNS service to manage your DNS records
-   E.g:Purchase the domain from Go Daddy and use Route 53  manage your DNS record

3rd Party Registar with amazon route 53
-   u buy your domain on 3rd party Registar,u use Route 53 as DNS service provider
    +   Create a Hosted zone in Route 53
    +   Update NS records on 3rd party website to use Route53 NAME SERVERS 
    +   DOMAIN REGISTRAR != DNS service
    +   But every Domain Registar usually comes with some DNS Features 

Keyword has remember
Section 11 : aws webflow pattern 
pattern 1 :
-   public vs Private IP and ec2
-   Elastic IP vs Route 53 vs Load balancer
-   Route 53 TTL , A records and alias records
-   Maintain ec2 intsance vs ASG (Auto scalling groups)
-   Multi AZ to survice disaster
-   ELB health checks
-   Security Group rules
-   Reservation of Capacity for costing savings when possible
-   considers 5 pillars for a well achitected application:
-   costs, performance , reliability , security , operational excellence
pattern 2 : 
-   ELB sticky session
-   Web client for storing cookies and making our web app stateless
-   ElastiCache
    +   For storing sessions 
    +   For caching data from RDS 
    +   Multi AZ
-   RDS
    +   For storing user data
    +   Read replica for scaling reads 
    +   Multi AZ for disater recovery
-   Tight Security with security group referencing each other 
pattern 3 : 
-   Aurora database to have easy multi az and read replica
-   Storing data in EBS (single instance app)
-   vs Storing data in EFS ( distribution app)


Instantiating app quickly
-   EC2 Instances : 
    +   USE GOLDEN AMI
    +   BOOTSTRAP USING USER DATA 
    +   HYBRID 
-   RDS databases : 
    +   Restore from a snapshot : the database will have schemas and data ready
-   EBS volumes:
    +   Restore from snapshot: the disk will already be formatted and have data!

Elastic Beanstalk - Overview
-   Elastic Beanstalk is a developer centric view of deploying application on AWS
-   It uses all the component's we've seen before : EC2, ASG , ELB, RDS , ...
-   Managed service
    +   Automatically handles capacity provisioning, load balancing, scaling
        application health monitoring , instance config ...
    +   Just the application code is the responsibility of the developer
-   We still have full control over the configuration
-   Beanstalk is free but you pay for the underlying instance

Elastic Beanstalk - components 
-   Application: collection of Elastic Beanstalk component
-   Application version : an iteration of your application code
-   Environments
    +   Collection of AWS resource running an application version 
    +   Tier : Web Server Environment Tier & Worker Environment Tier
    +   You can create multi environments

Elastic Beanstalk - support platforms : 
-   Go, JAVA SE, JAVA TOMCAT, net core linux, net win server,node, php, .py, ruby
-   Packer builder, single - multi container docker, pre config docker 
-   if not supported , you can write your custom platform (advanced )

Section 12 : Amazon s3 introduction 
Section introduction:
-   S3 is one of the main building block of AWS
-   it's advertised as " infinitely scaling " storage
-   it's widely popular and deserves its own section

Amazon s3 overview - Buckets
-   Amazon s3 allows people to store object (files) in "buckets" (directories)
-   Buckets must have a GLOBALLY UNIQUE NAME
-   Buckets are defined at the region level
-   Naming convention
    +   No uppercase
    +   No underscore
    +   3-63 character long 
    +   Not an IP 
    +   Must start with lowercase letter or number

Amazon S3 Overview - Objects
-   Objects (files) have a key
-   The key is the FULL path :
    +   s3://my-bucket/ğ¦ğ²_ğŸğ¢ğ¥ğ.ğ­ğ±ğ­
    +   s3://my-bucket/ğ¦ğ²_ğŸğ¨ğ¥ğğğ«/ğšğ§ğ¨ğ­ğ¡ğğ«_ğŸğ¨ğ¥ğğğ«/ğ¦ğ²_ğŸğ¢ğ¥ğ.ğ­ğ±ğ­
-   The key is composed of ğ—£ğ—¿ğ—²ğ—³ğ—¶ğ˜… + ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘›ğ‘ğ‘šğ‘’
    +   s3://my-bucket/ğ—ºğ˜†_ğ—³ğ—¼ğ—¹ğ—±ğ—²ğ—¿/ğ—®ğ—»ğ—¼ğ˜ğ—µğ—²ğ—¿_ğ—³ğ—¼ğ—¹ğ—±ğ—²ğ—¿/ğ‘šğ‘¦_ğ‘“ğ‘–ğ‘™ğ‘’.ğ‘¡ğ‘¥ğ‘¡

-   There's no conecpt of "directories" within buckets 
-   Just keys with very long names that contain slashes
-   Objects values are the content of the body:
    +   Max Objects Size is 5TB (5000GB)
    +   If uploading more than 5GB , must use "multi-part upload"
-   Metadata (list of text key / value pairs - system of user metadata)
-   Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
-   Version ID (if versioning is enabled)

Amazon s3 - Versioning 

-   You can version your files in Amazon s3
-   It is enabled at the BUCKET LEVEL
-   Same key overwrite will increment the "version" : 1,2,3...
-   It is best practice to version your buckets 
    +   Protect against unintended deletes 
    +   Easy roll back to previous version
-   Notes : 
    +   Any File that is not ver prior to enabling versioning will've ver "null"
    +   Suspending versioning does not delete the previous versions


Encrypt s3 bucket data 
SSE - S3
-   SSE - S3 : encryption using keys handled & managed by amazon s3
-   Object is encrypted server side
-   AES - 256 encryption type 
-   Must set header : "x-amz-server-side-encryption":"AES256"

SSE - KMS
-   SSE - KMS : encryption using keys handled & managed by KMS
-   KMS Advantages : user control + audit trail
-   Object is encrypted server side
-   Must set header : "x-amz-server-side-encryption":"aws:kms"

SSE - C
-   SSE - C : server-side encryption using data keys fully managed by the
    customer outside of AWS 
-   Amazon S3 does not store the encryption key you provide
-   HTTPS must be used 
-   Encryption key must provided in HTTP headers , for every HTTP req made

Client side encryption
-   Client library such as the Amazon s3 Encryption Client
-   Client must encrypt data themselves before sending to S3
-   Client must decrypt data themselves when retrieving from S3
-   Customer fully manages the keys and encryption cycle

Encryption in transit (SSL/TLS )
-   Amazon s3 exposes : 
    +   HTTP endpoint : non encrypted
    +   HTTPS endpoint : encryption in flight 
-   You're free to use the endpoint you want, but HTTPS is Recommended
-   Most clients would use the HTTPS endpoint by default 
-   HTTPS is mandatory for SSE-C 
-   Encryption in flight is also called SSL/TLS 

S3 Security
-   USER BASED 
    +   Iam policies - Which Api call should be allow a specific user from IAM 
-   RESOURCE BASED 
    +   Bucket Policies - bucket wide rules from s3 console - allow cross account
    +   Object access control list (ACL) - finer grain
    +   Bucket Access Control List (ACL) - less common
-   Not : an IAM principal can access an S3 object if 
    +   the user IAM permissions allow it OR the resource policy ALLOW it
    +   AND there's no explict DENY

S3 bucket policies
-   JSON based policies
    +   Resource : bucket and object
    +   Actions : Set of API to Allow or Deny
    +   Effect: ALlow / Deny
    +   Principal : the account or user to apply the policy to 
-   Use S3 bucket for policy to : 
    +   Grant public access to the bucket 
    +   Force object to be encrypted at upload
    +   Grant access to another account 

Bucket settings for BLock Public Access 
-   block public access to buckets and objects granted through 
    +   new access control lists (ACLs)
    +   any access control lists (ACLs)
    +   new public bucket or access point policies 
-   Block public and cross account access to buckets and objects through any
    public bucket or access point policies
-   These settings were created to prevent company data leaks
-   If you know your bucket should be public , leave these on 
-   Can be set at the account level

S3 Security - Other 
-   Networking :
    +   Supports VPC endpoints 
-   Loggin and Audit :
    +   S3 Access logs can be stored in other S3 bucket 
    +   API calls can be logged in AWS CloudTrail
-   User Security
    +   MFA Delete : MFA can be required in versioned buckets to delete objects
    +   Pre-Signed URLs: URLs that are valid only for a limited time 

S3 Websites
-   S3 can host static websites and have them accessiable on the www
-   The website URL will be 
    +   <bucket-name>.s3-website-<AWS-region>.amazonaws.com
-   If u get a 403 (forbidden) err,make sure the bucket policy allows public reads 

CORS - Explained
-   An origin is a scheme (protocol) , host (domain) and port 
    +   E.g: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
-   CORS means Cross - Origin Resource Sharing
-   WEB BROWSER based mechanism allow request other origin while visit main origin
-   Same origin : hÌµtÌµtÌµpÌµ:Ìµ/Ìµ/ÌµeÌµxÌµaÌµmÌµpÌµlÌµeÌµ.ÌµcÌµoÌµmÌµ/app1 & hÌµtÌµtÌµpÌµ:Ìµ/Ìµ/ÌµeÌµxÌµaÌµmÌµpÌµlÌµeÌµ.ÌµcÌµoÌµmÌµ/app2
-   Different origin : hÌµtÌµtÌµpÌµ:Ìµ/Ìµ/ÌµeÌµxÌµaÌµmÌµpÌµlÌµeÌµ1Ìµ.ÌµcÌµoÌµmÌµ  & hÌ¶tÌ¶tÌ¶pÌ¶:Ì¶/Ì¶/Ì¶oÌ¶tÌ¶hÌ¶eÌ¶rÌ¶.Ì¶eÌ¶xÌ¶aÌ¶mÌ¶pÌ¶lÌ¶eÌ¶1Ì¶.Ì¶cÌ¶oÌ¶mÌ¶
-   The request won't be fulfilled UNLESS the other origin allows for the 
    request , using CORS HEADERS ( vd : ACCESS-CONTROL-ALLOW-ORIGIN)

S3 CORS 
-   If a client does a cross-origin req on s3 bucket, we need enabled CORS header
-   It's a popular exam question
-   You can allow for a specific origin or for * (all origins , all method)

Amazon s3 - Consistency Model
-   Strong consistency as of December 2020
-   After a:
    +   Successful write of a new object (new PUT)
    +   or an overwrite or delete of an existing object (overwrite PUT or DELETE)
-   ...any:
    +   Subsequent read request immediately RECEIVES LATEST VERSION of the object
    +   Subsequent list request immediately reflects changes
-   Available at no additional cost without any performance impact

AWS EC2 Instance Metadata
-   AWS EC2 Instance Metadata is powerful but one of least known feat to dev
-   It Allows AWS EC2 instances to "learn about themselves" without using an 
    IAM ROles that purpose
-   The URL is ğŸ­ğŸ²ğŸµ.ğŸ®ğŸ±ğŸ°.ğŸ­ğŸ²ğŸµ.ğŸ®ğŸ±ğŸ°/ğ—¹ğ—®ğ˜ğ—²ğ˜€ğ˜/ğ—ºğ—²ğ˜ğ—®-ğ—±ğ—®ğ˜ğ—®
-   u can retrieve IAM Role name from metadata, but u CAN"T retrieve IAM policy
-   Metadata = info about the EC2 instance
-   Userdata = launch script of the EC2 instance

AWS SDK Overview 
-   What if u want perform actions on AWS directly from your app code (use CLI)
-   U can use an SDK
-   Offical SDKs are ...
    +   JAVA,Net,Node,PHP,PYTHON (named boto3 / botocore),GO,RUBY,C++ 
-   We have to use AWS SDK when coding against AWS Services such as DynamoDB
-   Fun fact... the AWS CLI uses the PYTHON SDK (BOTO3)
-   The exam expects you to know when you shoule use an SDK
-   We will preactice the AWS SDK when we get to LAMBDA FUNCTION
-   Good to know: if u don't specify or config default region => us-east-1

Section 14: S3 MFA- Delete 
-   MFA (multi factor authentication) forces user to generate a code on a device
    (usually a mobile phone or hardware) before doing important operations on S3
-   To use MFA - Delete , enabled versioning on the s3 bucket
-   You will need MFA to 
    +   permanently delete an object version
    +   suspend versioning on the bucket
-   You won't need MFA for 
    +   enabling versioning 
    +   listing deleted versions 
-   ONLY THE BUCKET OWNER (ACC ROOT) can enable/disable MFA-Delete
-   MFA - Delete currently can only be enabled using the CLI
 
 S3 Default Encryption vs Bucket Policy
-   One way to "force encryption" is to use a bucket policy and refuse any API
    call to PUT an S3 object without encryption header
-   Another way is to use the "default encryption " option in s3
-   Note : Bucket Policies are evaluated before default encryption

S3 Access Logs 
-   For audit purpose , you may want to log all access to s3 buckets 
-   Any request made to S3 , from any account , authorzied or denied
    will be logged into another s3 bucket 
-   That data can be analyzed using data analysis tools ...
-   Or amazon athena as we'll see later in this section 
-   The log format is at : 
    ğ—µğ˜ğ˜ğ—½ğ˜€://ğ—±ğ—¼ğ—°ğ˜€.ğ—®ğ˜„ğ˜€.ğ—®ğ—ºğ—®ğ˜‡ğ—¼ğ—».ğ—°ğ—¼ğ—º/ğ—”ğ—ºğ—®ğ˜‡ğ—¼ğ—»ğ—¦ğŸ¯/ğ—¹ğ—®ğ˜ğ—²ğ˜€ğ˜/ğ—±ğ—²ğ˜ƒ/ğ—Ÿğ—¼ğ—´ğ—™ğ—¼ğ—¿ğ—ºğ—®ğ˜.ğ—µğ˜ğ—ºğ—¹

S3 Access Logs : Warning 
-   Do not set your logging bucket to be the montiored bucket 
-   It will create a logging loop, and ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¯ğ˜‚ğ—°ğ—¸ğ—²ğ˜ ğ˜„ğ—¶ğ—¹ğ—¹ ğ—´ğ—¿ğ—¼ğ˜„ ğ—¶ğ—» ğ˜€ğ—¶ğ˜‡ğ—² ğ—²ğ˜…ğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ğ—¹ğ˜†

S3 Replication (CRR - SRR)
-   ğ— ğ˜‚ğ˜€ğ˜ ğ—²ğ—»ğ—®ğ—¯ğ—¹ğ—² ğ˜ƒğ—²ğ—¿ğ—¶ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ ğ—¶ğ—» ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—®ğ—»ğ—± ğ—±ğ—²ğ˜€ğ˜ğ—¶ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—» 
-   Cross Region Replication ( CRR )
-   Same Region Replication ( SRR )
-   Buckets can be in different accounts 
-   Copying is ASYNC
-   Must give proper IAM permisson to S3 
-   ğ˜¾ğ™ğ™ - ğ™ğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™šğ™¨ : compliance, lower latency access, replication across acc
-   ğ™ğ™ğ™ - ğ™ğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™šğ™¨ : log aggregation , live replication between
    production and test account 

S3 Replication - Notes 
-   After activating , only new objects are replicated 
-   Optionally, you can replicate existing objects using S3 Batch Replication
    +   Replicates existing object and objects that failed replication 
-   For DELETE operations : 
    +   Can replicate delete markers from source to target ( optional setting )
    +   Deletions with a version ID are not replicated (avoid malicious deletes)
-   There is no " chaning " of replication 
    +   if bucket I has replication into bucket 2, which has replication into b3 
    +   then object created in bucket are not replication to b3 

S3 pre-signed URL
-   Can generated using SDK or CLI 
-   For downloads ( CLI )
-   For uploads ( must using SDK )
-   Valid for default 3600s , can change timeout with --expries-in TIME_BY_S arg 
-   Users given pre-signed URL inherit permission of persion who Generate URL
    for GET - PUT 
-   Examples : 
    +   Allow only logged in - users to download previum video on your s3 bucket
    +   Allow an ever changing list of users to download files by gen URL dynamic
    +   Allow temp a user upload a file to precise loaction on your bucket 

S3 Storage Classes 
-   Amazon S3 Standard - General Purpose 
-   Amazon s3 Standard - Infrequent Access 
-   Amazon s3 One Zone infrequent Access 
-   Amazon s3 Glacier Instant Retrieval
-   Amazon s3 Glacial Flexible Retrieval
-   Amazon s3 Glacial Deep Archive
-   Amazon s3 interlligent Tiering 
-   Can more between class manually or using s3 LIFECYCLE CONFIG

S3 Durability and Avalability
-   Durability:
    +   High durability (99.99% || 9's) of object accross multi AZ 
    +   If you store 10 milion object with S3 , u can average expect to incur
        a loss of single object ones every 10k years 
    +   Same for all storage class
-   Avalability
    +   Measures how readily Available a service is 
    +   Varies depending on storage class 
    +   Ex : S3 Standard has 99.99% availability = not 53 minutes / year (err) 

S3 Standard - General Purpose 
-   99.99% availability
-   User for requently access data
-   low latency and high thoughput 
-   Sustain 2 concurrent failure 
-   UseCase : Big Data anylitics , mobile - gaming app , content distribution...

S3 Storage access - Infrequent access 
-   For data that is less frequently required  ,but requires access when need 
-   Lower cost than S3 Standard

-   ğ—”ğ—ºğ—®ğ˜‡ğ—¼ğ—» ğ˜€ğŸ¯ ğ—¦ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—± ğ—¶ğ—»ğ—³ğ—¿ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ˜ ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ (ğ—¦ğŸ¯ ğ—¦ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—± -ğ—œğ—”)
    +   99.9% availability
    +   Use Cases : Disaster Recovery, backups 
-   ğ€ğ¦ğšğ³ğ¨ğ§ ğ¬ğŸ‘ ğğ§ğ ğ™ğ¨ğ§ğ ğˆğ§ğŸğ«ğğªğ®ğğ§ğ­ ğ€ğœğœğğ¬ğ¬ ( ğğ§ğ ğ™ğ¨ğ§ğ - ğˆğ€)
    +   High durability (99.99%) in a single AZ ; data lost when AZ is destroyed
    +   99.95 availability
    +   Use cases : Story secnondary backup copies of on - premise data 
        or data you can recreate  
-   Amazon s3 Glacier Storage Classes 
    +   Low - cost object storage meant for archiving / backup
    +   Pricing : price for storage + object retrieval cost 
-   Amazon S3 Glacier Instant Retrieval 
    +   Milisecond retrieval , great for data accessed once a quarter
    +   Minium storage duration of 90 days
-   Amazon S3 Glacier Flexible Retrieval (formerly Amazon s3 Glacier)
    +   Expedited (1 to 5 min), Standard (3 to 5 hr),Bulk (5 to 12hr) - free
    +   Minium storage duration of 90 days
-   Amazon S3 Glacier Deep Archive - for long term to storage 
    +   Standard ( 12hr ) , Bulk (48hr )
    +   Minium storage duration of 180 days

S3 Interlligent - Tiering 
-   Small monthly montioring and auto - tiering fee 
-   Moves objects automatically between Access Tier based on usuage
-   There are no retrieval charges in S3 Interligent - Tiering 
-   Frequent Access tier (automatic): default tier 
-   Infrequent Access tier (automatic): objects not accessed for 30days 
-   Archive Instant Access Tier(automatic) : objects not accessed for 90days 
-   Archive Access tier : 90 > 700+ days
-   Deep Archive Access tier : 120 > 700+ days

S3 Lifecycle rules
-   ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—¶ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€: if defines when object're transitioned another storage class
    +   Move objects to Standard IA class 60 days after creation 
    +   Move to Glacier for archiving after 6 month
-   ğ—˜ğ˜…ğ—½ğ—¶ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ : configure object to expire (delete) after some time 
    +   access log file can be delete when out of 365 days 
    +   ğ—°ğ—®ğ—» ğ—¯ğ—² ğ˜‚ğ˜€ğ—²ğ—± ğ˜ğ—¼ ğ—±ğ—²ğ—¹ğ—²ğ˜ğ—² ğ—¼ğ—¹ğ—± ğ˜ƒğ—²ğ—¿ğ˜€ğ—¶ğ—¼ğ—» ğ—¼ğ—³ ğ—³ğ—¶ğ—¹ğ—² (ğ—¶ğ—³ ğ˜ƒğ—²ğ—¿ğ˜€ğ—¶ğ—¼ğ—»ğ—»ğ—¶ğ—»ğ—´ ğ—¶ğ˜€ ğ—²ğ—»ğ—®ğ—¯ğ—¹ğ—²ğ—± )
    +   Can be used to delete incomplete multi-part uploads
-   Rules can be created for a certain prefix (ex - S3://mybucket/mp3/*)
-   Rules can be created for certain objects tags (ex - Department : Finance)

S3 Lifecycle Rules - Scenario I
-   Your application on EC2 creates images thumbnails after profile photos are 
    Uploaded to Amazon s3 . These thumbnails can be easilly created , and only
    need to be kept for 45 days . The source images should be able to be
    immediately retrieved for these 45 days, and afterwards , the user can wait
    up to 6 hr . How would u design this ? 
-   Answer :
    +   S3 source images can be on STANDARD , with a lifecycle config transition
        them to GLACIER after 45 days 
    +   S3 thumbnails can be on ONEZONE_IA with a lifecycle config expire them
        after 45 days 
S3 Lifecycle Rules - Scenario 2
-   A rule in your company states that you sould be able to recover your deleted
    S3 objects immediately for 15 days, although this may appen rarely.After this
    time , And for up to 1 years, deleted object sould be recoveable with in 48 hr
    Solution : 
    +   You enabled s3 versioning in order to have object versions, so that 
        "delete objects" are in fact hidden by "delete marker" and can be recovered
    +   You can transition these "noncurrent versions" of the object to S3_IA
    +   You can transition afterwards these "noncurrent versions" to DEEP_ARCHIVE

S3 Analytics - Storage Class analysis
-   You can setup S3 analytics to help determine when to stransition objects from 
    Standard to Standard_IA
-   Does not work for ONEZONE_IA or GLACIER 
-   Report is updated daily 
-   Takes about 24h to 48hr to first start 
-   Good first step to put together Lifecycle Rules (or improve them)!

S3 - Baseline Performance 
-   Amazon s3 automatically scales to high request rates, latency 100-200ms 
-   Your application can archieve at least 3,500 PUT/COPY/POST/DELETE and
    5,500 GET/HEAD requests per second per prefix in a bucket
-   There are no limits to the number of prefixes in a bucket 
-   Example (Object path => prefix)
    +   bucket/folder1/sub1/file => folder1/sub1
-   If you spread reads across all four prefixes evenly, you can achieve 22,000
    requests per second for GET and HEAD 

S3 - KMS Limitation 
-   If you use SSE - KMS, you may be impacted by the KMS limits
-   When u upload, it calls the ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—²ğ——ğ—®ğ˜ğ—®ğ—ğ—²ğ˜† KMS API 
-   When u download, it calls the ğ——ğ—²ğ—°ğ—¿ğ˜†ğ—½ğ˜ KMS API 
-   Count towards the KMS quota per s (5500, 10k, 30k req/s on the region )
-   you can request a quota using Service Quota Console 

S3 Performance 
-   Multi-part upload 
    +   Recommended for files > 100MB, must use for files > 5GB
    +   Can help parallelize uploads (speed up transfers)
-   S3 transfer Acceleration 
    +   Increase transfer speed by transferring file to an AWS edge location which
        will forward the data to the s3 bucket in the target region
    +   Compatible with multi-part upload 

S3 Performance - S3 Byte -Range fetches 
-   parallelize gets by requesting specific byte ranges 
-   better resilience in case of failures 
-   CAN BE USED TO SPEED UP DOWNLOAD

S3 Select & Glacier Select 
-   Retrieve less data using SQL by performming server side filtering 
-   can filter by col & row ( simple SQL statements )
-   less network transfer , less cpu cost client side 

s3 Event Notification
-   s3:ObjectCreated , s3:ObjectRemoved , s3: ObjectRestore, s3:Replication
-   Object name filtering possible (*.jpg)
-   Usecase : generate thumbnauls of images uploaded to s3 
-   ğ—–ğ—®ğ—» ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—®ğ˜€ ğ—ºğ—®ğ—»ğ˜† "ğ˜€ğŸ¯ ğ—²ğ˜ƒğ—²ğ—»ğ˜ğ˜€" ğ—®ğ˜€ ğ—±ğ—²ğ˜€ğ—¶ğ—¿ğ—²ğ—± 
-   s3 event notification typlically deliver events in seconds but can 
    sometimes take a minite of longer 

s3 Event notifications with amazon EventBridge
-   Advanced filtering option with JSON rules 
-   multiple distinations
-   EventBridge Capacibilities

s3 Requester Pays 
-   In general , bucket owners pay for all Amazon s3 storage and data transfer
    const associated with thieir bucket
-   With requester Pays buckets, the requester instead of the bucket owner
    pay the const of req and the data download from the bucket 
-   helpful when u want to share large datasets with author account 
-   the quester muse e a auth 


Amazon athena 
-   SERVERLESS  query service to PERFORM ANALYTICS AGAINST S3 OBJECTS 
-   Uses standard SQL language to query the files 
-   Supports CSV, JSON, ORC, AVRO, and Parquet (built on Presto)
-   Pricing: $5.00 per TB of data scanned 
-   Use compressed or columnar data for cost savings (less scan)
-   Use cases: Business intelligence / analytics / reporting ,...

NOTE : ANALYZE DATA IN S3 USING SERVERLESS SQL , USE ATHENA

S3 Object Lock (versioning must be enabled)
-   Adopt a WORM (Write Once Read Many) model
-   Block an object version deletion for a specificd amout of time
-   Object retention:
    +   Retention Period : specifies a fixed period
    +   Legal Hold: same protection, no expiry date
-   Modes:
    +   Governance mode: users can't overwrite or delete an object version or
        alter its lock settings unless they have special permissions
    +   Compliance mode:a protected object version can't be overwritten or deleted
        by any user, including the root user in your AWS account. When an object
        is locked in compliance mode, its retention mode can't be changed, and its
        retention period can't be shortened

SECTION 15 : 
AWS Cloud Front 
-   (CDN) Content Delivery Network
-   Improves read performance, content is cached at the edge
-   216 Point of Presence globally (edge locations)
-   DDoS protection, integration with Shield, AWS Webapp firewall
-   can expose https and can talk to internal HTTPS backends 

CloudFront - Origins 
-   S3 bucket 
    +   for distributing file and caching them at the edge
    +   Enhanced security with Cloudfront OAI ( Origin Access Identity )
    +   Cloudfront canbe used as an as ingress (to upload files to s3 )
-   Custom origin(HTTP)
    +   ALB (app load balancer)
    +   ec2 instance 
    +   s3 website 
    +   Any HTTP backend u want 

CloudFront Geo Restriction
-   u can restrict who can access your distribution
    +   whitelist 
    +   blacklist 
-   The "country" is determined using a 3rd party Geo-IP database 

Cloudfront vs s3 cross region replication
-   CloudFront :
    +   Global edge network
    +   file are cached by TTL (maybe a day)
    +   ğ—šğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—¼ğ—¿ ğ˜€ğ˜ğ—®ğ˜ğ—¶ğ—° ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜ ğ˜ğ—µğ—®ğ˜ ğ—ºğ˜‚ğ˜€ğ˜ ğ—¯ğ—² ğ—®ğ˜ƒğ—®ğ—¶ğ—¹ğ—®ğ—¯ğ—¹ğ—² ğ—²ğ˜ƒğ—²ğ—¿ğ˜†ğ˜„ğ—µğ—²ğ—¿ğ—²
-   S3 Cross Region Replication : 
    +   Must be setup for each region you want replication to happens
    +   Files are updated in near real-time 
    +   Read only
    +   ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—¼ğ—¿ ğ—±ğ˜†ğ—»ğ—®ğ—ºğ—¶ğ—° ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜ ğ˜ğ—µğ—®ğ˜ ğ—»ğ—²ğ—²ğ—±ğ˜€ ğ˜ğ—¼ ğ—¯ğ—² ğ—®ğ˜ƒğ—®ğ—¶ğ—¹ğ—®ğ—¯ğ—¹ğ—² ğ—®ğ˜ ğ—¹ğ—¼ğ˜„-ğ—¹ğ—®ğ˜ğ—²ğ—»ğ—°ğ˜† ğ—¶ğ—» ğ—³ğ—²ğ˜„ ğ—¿ğ—²ğ—´ğ—¶ğ—¼ğ—»ğ˜€

CloudFront Signed URL / Signed Cookies 

-   You want to distribute paid shared content to premium user over the world 
-   we can use CloudFront sign URL / Cookie . We attach a policy with: 
    +   Includes URL expiration
    +   include ip range to access data from 
    +   trusted signers 
-   how long should url be valid for ? 
    +   share content : make it short 
    +   private content : you can make it last for years 
-   Signed url = access to individual files 
-   signed cookies = access to multiple files 

CloudFront Signed URL vs S3 pre-signed URL 
-   Cloudfront Signed URL :
    +   Allow access to a path, no matter the origin
    +   Account wide key-pair , only the root can manage it 
    +   Can filter by ip, path , date, expiration
    +   can leverage caching features 
-   s3 pre-signed url 
    +   Issue a request as the person who pre-signed the URL
    +   Uses the IAM key
    +   limited lifetime 

CloudFront - Price Classes 
-   you can reduce the number of edge location for COST REDUCTION
-   Three price classes 
    +   100  :  only the least exprensive regions 
    +   200  :  most regions , but excludes the most exprensive regions 
    +   all  :  all regions -> best performance

CloudFront - multiple Origin 
-   To route to different kind of origins based on the content type 
-   Based on path pattern 
    +   /images/*

CloudFront - Origin Groups 
-   To increase high - availability and do failover
-   origin group : one primary and one  secnondary origin
-   if the primary origin fail , use second origin

CloudFront - Field Level Encryption
-   protect user sensitive infor thorugh app stack
-   adds an additional layer of security along with https 
-   sensitive information encrypted at the edge close to user 
-   uses asymmetric encryption
-   Usage : 
    +   Specify set of fields in POST request that u want to be encrypted 
    +   specify to public key to encrypted them

Global users for our application 
-   You have deployed an application and have global user who want access
-   They go over public internet, which can add litte latency due many hops
-   We wish to go as fast as possible through AWS network minimize latency

Unicast IP and Anycast IP 
-   Unicast IP : one server holds one ip address
-   Anycast IP : all servers holds the same ip client routes nearest one 

AWS Global Accelerator 
-   Leverage the AWS internal network to route to your application
-   2 Anycast IP create for your application
-   The Anycast IP send traffic directly to Edge Locations 
-   The Edge locations send the traffic to your application

AWS Global Accelerator 
-   Works with ğ—˜ğ—¹ğ—®ğ˜€ğ˜ğ—¶ğ—° ğ—œğ—£ , ğ—˜ğ—–ğŸ® ğ—¶ğ—»ğ˜€ğ˜ğ—®ğ—»ğ—°ğ—²ğ˜€ , ğ—”ğ—Ÿğ—• , ğ—¡ğ—Ÿğ—•, ğ—½ğ˜‚ğ—¯ğ—¹ğ—¶ğ—° ğ—¼ğ—¿ ğ—½ğ—¿ğ—¶ğ˜ƒğ—®ğ˜ğ—² 
-   Consistent performance
    +   intelligent routing to lowest latency and fast regional failover 
    +   no issues with client cache 
    +   Internal aws network
-   Health Checks 
    +   Global Accelerator perform a health check of your application
    +   Helps make your application global
    +   Great for Disaster recovery
-   Security
    +   only 2 external IP need to be whitelisted 
    +   DDOS protection thanks to AWS shield 

AWS Global Accelerator vs CloudFront 
-   They both use AWS GLOBAL NETWORK and its edge location arround the world 
-   Both services intergrate with AWS Shield for DDOS protect

-   CloudFront :    
    +   Improves performance for both cacheable content 
    +   Content is served at the edge 
-   Global Accelerator
    +   Improves performance for a wide range for application over TCP or UDP
    +   Proxying packets at the edge to app running in 1 of more aws regions 
    +   Good fit for non-HTTP use cases, such as gaming (UDP),IOT,Voice over IP
    +   Good for HTTP use case that requies static ip address 
    +   Good for HTTP use case that required deterministic fast regional failover
    