

    SECTION 8  (continue )
    LEARNING AWS
Iam : Permissions : 

- Users or groups can be assigned JSON documents called policies
- These policlies define the permissions of the users 
- In AWS you apply the least privilege principle  : don’t give more
Permissions than a user needs

IAM Policies Structure
Consists of 
-	Version : policy language version 
-	Id : an identifier for the policy
-	Statement : one or more individual statements (required)
Statements consists of 
-	Sid : an identifier for the statement (optional)
-	Effect : whether the statement allows or denies access ( Allow , Deny)
-	Principal : account/user/role to which this policy applied to
-	Action : list of actions this policy allows or denies
-	Resource :list of resources to which the actions applied to
-	Condition : conditions for when this policy is in effect (optional)

IAM – PASSWORD POLICY
2 Defense mechanisms : 
-	Strong passwords = higher security for your account 
-	In AWS , you can setup a password policy

o	Set a minium password length
o	Require specific character types
	Including uppercase letters 
	Lowercase letters
	Number , or non alphanumeric character (ki tu dac biet )
o	Allow all IAM users to change their own passwords
o	Require user to change their password after some time (password exp)
o	Prevent password re-use
Multi Factor Authen – MFA
-	Users have acess to your account and can possibly change config or delete
Resources in your AWS account 
-	You want to protect your Root Accounts and IAM users
-	MFA = password you know + security device you own

Section 8
•	Application load balancer (v2)
•	Application load balancers is Layer 7 (HTTP)
•	Load balancing to multiple HTTP app (target groups)
•	Load balancing to multiple applications on the same machine 
•	Support for http/2 and websocket
•	Support redirects (http/https)
•	Routing tables to different target groups:
o	Routing path url
o	Routing hostname url
o	Routing on query string, header 
•	ALB are a great fit for micro and container app
•	Has a port mapping feature to redirect to dynamic port in ecs
•	In comparsion , we’d need multiple classic load balancer per application 
 
Application load balancer (v2) target group

•	EC2 instances (can be managed by an auto scaling group) – HTTP
•	ECS task (managed by ecs it self) -HTTP
•	Lambda function – HTTP req/ JSON event
•	Private IP address
•	ALB can route to multiple target group
•	Health checks are at the target group level

ALB (v2) Good to know
•	Fixed hostname 
•	the application servers don’t see the IP client 
	inserted in the header X-forwarded-for
Network Load Balancer (ELB v2)
•	Network load balancers (Layer 4 ) allow to
o	Forward TCP & UDP traffic to your instances
o	Handle millions of request per seconds 
o	Less latency ~100ms (vs 400 ms for ALB)
•	NLB as one static IP per AZ, and supports assigning Elastic IP
•	NLB as used for extreme performance , TCP or UDP traffic
•	Not free 

Gateway Load Balancer
•	Deploy scale , and manage a fleet of 3rd party network
•	Operates Layer 3 ( network layer ) – IP packets 
•	Combines the following function : 
o	Transparent network gateway
o	Load balancer
•	Uses the GENEVE port 6081
•	Ec2 instance
•	Private IP address
Sticky Sessions (Session Affinity)
•	This works for Classic Load balancers & application Load balancer
•	The “cookie” used for stickiness has an exp date
•	Enabling stickness may bring imbalance to the load over the be EC2
Sticky Sessions – Cookie Names 
•	Application-based Cookie
o	Custom cookie
	Don’t use AWSALB , AWSALPAPP , AWSALBTG
o	Application cookie
	As AWSALBAPP
•	Duration-based cookie
o	Cookie generated by the load balancer
o	Cookie name is AWSALB for alb , AWSELB

Cross-Zone Load balancing
•	Application Load Balancer
o	Always on 
o	No changes for inter AZ
•	Network Load Balancer
o	Default as disabled
o	Pay charges 
•	Classic Load Balancer
o	Disabled by default
o	No charges for inter AZ





Section 9 : 
SSL - Basics 
-   An ssl Certificate allows traffic between your clients and your load balancer
to be encrypted in transit 
-   SSL refer to Secure Sockets Layer , used to Encrypt connections 
-   TLS refers to transport Layer security , which is a newer version
-   Nowadays , TLS cert are mainly used , but SSL refer
-   Public ssl cert ase issues by (CA)
-   SSL cert have an exp ( you set ) and must be renewed
Load Balancer - SSL Cert 
-   The load Balancer uses an X509 cert (SSL/TLS)
-   You can manage cert usign ACM
-   You can create upload your own cert alternatively
    HTTPS Listener : 
    +   You must specify a default cert
    +   add an option of cert when support multi domain
    +   Clients can use SNI(Server Name indi) to specify hostname
    +   ability to specify a security policy to support SSL/TLS

SSL -  Server Name Indication
-   SNI solves the problem of loading multi SSL cert onto one web Server
-   It's a "newer" protocol, and requies the client to indicate the hostname
    of the target server in the initial SSL 
-   The server will then find the correct cert , of return the default one
    Note : 
        +   Only work for ALB-NLB
        +   Does not work for CLB
ELB - SSL cert
-   Classic Load Balancer (v1)
    +   support only one SSL cert
    +   Must use multi CLB for multi host with multi cert
-   Application Load Balancer (v2)
    +   Support multi Listener with multi SSL cert
    +   Uses SNI to make it work
-   Network Load Balancer (v2)
    +   Support multi Listener with multi cert 
    +   Users SNI

Connection Draining
-   Feature naming
    +   Connection Draining : CLB
    +   Deregistration : ALB - NLB
-   Time to complete "in flight request " while the instance is 
    de-registering or unhealthy
-   Stops sending new request to ec2 instance which ism  de-registering
-   Between 1 to 3600 sec (default as 300 , 0 as disabled)
-   Set to a low value if your request are short 
Auto scalling Group? 
-   The goal of Auto Scalling Group(ASG) is to :
    +   Scale out (add EC2 instance) to match an increased Load
    +   Scale in (remove EC2 instance) to match a decrease load 
    +   Ensure we have a minium and a maximum number of ec2 instances running
    +   Auto matically register new instances to a load balancer 
    +   Re-create an EC2 instance in case a previous one is terminated
-   ASG are free ( but as pay for the underlying EC2 )
Auto Scalling Group Attr
-   A Lunch Template 
    +   AMI  and Instance Type 
    +   EC2 User Data
    +   EBS volumes
    +   Security Groups
    +   SSH key pair 
    +   IAM roles for Your EC2 instances
    +   Network + Subnet Infor 
    +   Load Balancer infor 
-   Minsize / Maxsize / Intial Capacity
-   Scaling Policies
Auto Scalling - CloudWatch Alarams and Scalling 
-   It is possible to scale an ASG based on CloudWatch alarms 
-   An Alarm monitors a metric (such as Average CPU , or a custom metric)
-   Metrics such as Average CPU are computed for the overall ASG instances
-   Based on the alarm : 
    +   We can create scale-out policies (increase  number instance)
    +   We can create scale- policies (decrease  number instance)

ASG - Dynamic Scalling Policies
-   Target Tracking Scaling
-   Simple/Step Scalling 
    +   When a Cloudwatch alarm is trigger ( >  : add unit )
    +   When a Cloudwatch alarm is trigger ( <  : remove)
-   Scheduled Application  
    +   Anticipate a scaling based on known usage pattern 

ASG - Predictive Scalling 
    +   Predictive scalling : continuously forecast load and schedule scaling ahead

ASG - Good metrics 
    -   CPU Utilization : Average CPU utilization across your instances 
    -   RequestCountPerTarget : to make sure the number of requests per EC2 instances is stable 
    -   Average Network I/O 
    -   Any custom metric (using Cloudwatch)
ASG - Scalling Coldowns
-   After a scaling activity happens, you are in the 
    cooldown period (default 300s )
-   During the coldonw period, the ASG will not 
    launch or terminate additional instances
-   Advice : Use a ready-to-use API to reduce config time in 
    order to be serving req faster and reduce the cooldown period

ASG for SA
-   ASG default Termination Policy 
    1 : find the az which has the most number of instance
    2 : if there are multi instance in the az to choose from,
    delete the one with the oldest launch config
-   ASG tries the balance the number of instances across AZ by default 

ASG for SA - Lifecycle hook 
-   By default as soon as an instance is launched in an ASG it's in service
-   You have the ability to perform extra steps before instance goes in service
-   you have the ability to perform some actions before instance is terminated

ASG for SA - Launch Template vs Launch configurations
-   Both:
    +   ID of the Amazon Machine Image (AMI) , the intsance type , a key pair 
    security groups and the other param(e.g : ec2 user-data) that you
    use to launch EC2 instance 
-   Launch Config (legacy)
    +   Must be re-created every time 
-   Launch Template (newer)
    +   Can have multiple versions
    +   Create pram subsets
    +   Provision using both On-Demand and Spot instances
    +   Can use T2 unlimited burst Feature
    +   Recommended by AWS going forward

Section 9 : 
AWS RDS Overview
-   RDS stands for Relational Database service
-   it's a managed DB service for DB use SQL as a query language
-   It allows you to create databases in the cloud that are managed by AWS
    +   Postgres
    +   MySQL
    +   MariaDB
    +   Oracle
    +   Microsoft SQL Server
    +   Aurora (not critial remember it )

Advantage over using RDS versus deploying DB on ec2
-   RDS us a managed service:
    +   Automated provisioning , OS patching 
    +   Continuous backups and restore to specific timestamp
    +   Monitoring dashboards 
    +   Read replicas for improved read performance
    +   Multi AZ setup for DR (Disaster Recovery)
    +   Scaling capability (vertical and horizontal)
    +   Storage backed by EBS (gp2 or io1)
-   BUT you CAN't SSH into your instances

RDS backups
-   Backups are automatically enabled in RDS
-   Automated backups: 
    +   Daily full backup of the database 
    +   Transaction logs are backed-up by RDS every 5 minutes
    => ability to restore to any point in time (5 min)
    +   7 days retention (can be increased to 35 days )
-   DB Snapshots:
    +   Manually triggered by the user
    +   Retention of backup for as long as you want

RDS -  Storage Auto Scalling 
-   Helps you increase Storage on your RDS DB instance dynamically
-   When RDS detects you are running out of free database storage, it scales auto
-   Avoid manually scaling your database storage
-   you have to set MAXIMUM STORAGE THRESHOLD 
-   Automatically modify storage if :
    +   Free storage is less than 10% of allocated storage
    +   Low-storage lasts at least 5 min
    +   6 hours have passed since last modification
-   Useful for appications with unpredictable workloads
-   Supports all RDS database engines

RDS Read Replicas for read scalability
-   UP TO 5 READ REPLICAS
-   With in AZ, Cross Az or Cross Region
-   Replication is ASYNC so read are eventtually consistent
-   Replicas can be promoted to their own DB

RDS Read Replicas - Use Cases 
-   You have o production database that is taking on normal load
-   You want to run a reporting application to run some analytics
-   You create a read replica to run the new workload there
-   The production application is unaffected
-   Read replicas are used for SELECT (=read) only kind of statements
    (not INSERT, UPDATE , DELETE)

RDS - Network Cost

-   same AZ with other region : free (vd : us-east-1a <> us-east-1b)
-   other AZ same region : not free (vd : us-east-1a <> eu-west-1b)

RDS Multi AZ (Disaster Recovery)
-   SYNC Replication
-   One DNS name - automatic app failover to standby
-   Increase availability
-   Failover in case of loss of AZ, loss of network, instance or storage failure
-   No manual intervention in apps
-   Not used for scaling
+   Note : The Read Replicas be setup as MULTI AZ for Disaster Recovery

RDS - FROM Single-AZ to multi AZ
-   Zero downtime operation
-   Just click on "modify" for the database config
-   The following happens internally
    +   A snapshot is taken
    +   A new DB is restored from the snapshot in a new AZ 
    +   Synchonization is established between the two databases 

RDS Security - Encryption
-   At rest encryption 
    +   Possibility to encrypt the master and replicas with AWS-AES 256
    +   Encryption has to be defined at lunch time
    +   If the master is not encrypted , the read replicas cannot be encrypted
    +   Transparent Data Encryption (TDE) available for Oracle and SQL Server
-   In Flight encryption
    +   SSL cert options with trust cert when connecting to database
    +   Provide SSL options with trust cert when connecting to database 
    +   To enforce SSL:
        *   PostgreSQL : _ssl = l in the AWS RDS Console
        *   My SQL : with in the DB  .... REQUIRE SSL
RDS Encryption operations 
-   Encrypting RDS Backups
    +   Snapshots of un-encrypted RDS databases are un-encrypted
    +   Snapshots of encrypted RDS databases are encrypted
    +   Can copy a snapshot into an encrypted one 
-   To encrypt an un-encrypted RDS database 
    +   Create a snapshot of the un-encrypted database
    +   Copy the snapshot and enable encryption for the snapshot
    +   Restore the database from the encrypted snapshot
    +   Migrate applications to the new database , and delete the old database

RDS Security - Network & IAM
-   Network Security
    +   RDS databases are usually deployed within a private subnet, not a public one
    +   RDS security work by leveraging security groups- it controls
    with IP/Security group can COMMUNICATE with RDS 
-   Access Management 
    +   IAM policies help control who can manage AWS RDS
    +   Traditional Username and Password can be used to login into the database 
    +   IAM-based authen can be used to login into RDS MySQL  & PostgreSQL

RDS - IAM authen
-   IAM database authen work with MYSQL AND POSTGRESQL
-   You don't need a password, just an authen token obtained thorugh IAM - RDS API
-   Auth token expired when after 15min
-   Benefits : 
    +   Network in/out must be encrypted using SSL
    +   IAM to centrally manage users instead of DB 
    +   Can Ieverage IAM Roles and EC2 Instance profiles for easy integration 

RDS Security - Summary 
-   Encryption at rest
    +   Is done only when you first create the DB instance
    +   or : unencrypted DB => snapshot => copy snapshot as encrypted
        => create DB from snapshot 
-   Your responsibility
    +   Check the ports / IP / Security group inbound rules in DB's SG
    +   In-database user creation and permissions or manage through IAM
    +   Creating a database with or without public acess 
    +   Ensure param group or DB is configured to only allow SSL 
-   AWS responsibility
    +   No SSH access 
    +   No manual DB patching 
    +   No manucal OS patching 
    +   No way to audit the underlying instance

RDS Security - encryption
-   At rest encryption
    +   Possibility to encrypt the master & the read replicas with AWS KMS - AES - 256 encryption
    +   Encryption has to be defined at lunch time 
    +   If the master is not encrypted , the read replicas CANNOT be encrypted
    +   Transparent Data Encryption(TDE) available for Oracle and SQL Server
-   In-flight encryption
    +   SSL cert to encrypt data to RDS in flight 
    +   Provide SSL options with trust cert when connecting to database 
    +   To enforce SSL : 
        *   PostgreSQL : as Params Group ( rds.force_ssl  = 1 )
        *   MySQL : with in the DB query  ( ...... + REQUIRE SSL)

RDS Encryption operation
-   Encrypting RDS backup
    +   Snapshots of un-encrypted RDS database are un-encrypted
    +   snapshot of encrypted RDS database are encrypted
    +   Can copy a snapshot into an encrypted one
-   To encrypt an un-encrypted RDS database 
    +   Create a snapshot of the un-encrypted database
    +   Copy a snapshot and enable encryption for the snapshot
    +   Restore the database from the encrypted snapshot
    +   Migrate application to the new database, and delete the old database

RDS Security - Network & IAM
-   Network security
    +   RDS database are usually deployed within private subnet,not a public one
    +   RDS security work by leveraging SECURITY GROUPS - it controls which IP /
        Security group can COMMUNICATE with RDS
-   Access Management
    +   IAM policies help control who can MANAGE AWS RDS (through the RDS API)
    +   Traditional Username and Password can be used to LOGIN INTO the database
    +   IAM-based authen can be used to login into RDS MYSQL & PostgreSQL

RDS - IAM Authen
-   IAM database authen work MYSQL - PostgreSQL
-   You don't need a password,just an authen token obtained through IAM-RDS called
-   Auth token expried than 15min
-   Benefits:
    +   Network i/o must be encrypt using SSL
    +   IAM to centrally manage users instead of DB
    +   Can leverage IAM ROles and EC2 Instance profiles for easy integration


RDS Security - Summary 
-   Encryption at rest : 
    +   Is done only when you first create the DB Instance
    +   Or : unencrypted DB => snapshot => copy snapshot as encrypted
        => Create DB from snapshot
-   Your responsibility
    +   Check the ports/ IP / security group inbound rules in DB's SG
    +   In-database user creation and permissions or manage through IAM
    +   Creating a database with or without public access
    +   Ensure param group or DB is config to only allow SSL connection
-   AWS responsibility
    +   No SSH access
    +   No manual DB patching 
    +   No manual OS patching
    +   No way to audit the underlying instance

Amazon Aurora 
-   is proprietary from AWS (not open source)
-   Postgres and MYSQL are both supported as Aurora DB
-   Aurora is "AWS cloud optimized" and claims 5x perform improvement over MYSQL
    on RDS , over 3x the performance of Postgres on RDS
-   Aurora storage automatically grows in increments of 10gb , upto 128gb
-   Aurora can have 15 replicas while MYSQL as 5,and the replication process faster
-   Failover in Aurora is instantaneous. It's HA native
-   Aurora costs more than 20% RDS - but is more efficient 

Aurora high availability and read scaling 
-   6 copies of your data across 3az
    +   3 copies out of 6 need for read
    +   self healing with peer-to-peer replication
    +   Storage is striped across 100s of volumes
-   One Aurora instance takes writes ( master )
-   Automated failover for master in less than 30s 
-   master + up to 15 Aurora Read Replicas serve needs
-   Support for cross region replication

Feature of Aurora
-   Automatic failover
-   Backups and Recovery
-   Isolation and security
-   Industry compliance
-   Push - button scaling 
-   Automated Patching with Zero Downtime
-   Advanced Monitoring 
-   Routine Maintenance
-   Backtrack: restore data at any point of time without using backup

Aurora security
-   Similar to RDS b.c uses the same engines
-   Encryption at rest using KMS
-   Automated backups , snapshots and replicas are also encrypted
-   Encryption in flight using SSL 
-   POSSIBILITY TO AUTHEN USING IAM TOKEN 
-   You are responsible for protecting the instance with security groups 
-   you can't SSH

Aurora Serverless 
-   Automated database instantiation and auto - scaling based on actual usage
-   good for infrequent , intermittent or unpredictable workload
-   no capability planning need
-   pay per seconds , can be more cost effect
Global Aurora 
-   Aurora Cross Region read replicas 
    +   Useful for disaster Recovery
    +   Simple to put in place
-   Aurora Global database (Recommended)
    +   I primary Region (read/write)
    +   Up to 5s (read-only) regions , replication lag is less than I second
    +   Up to 15 Read replica per second region
    +   Helps for decrasing latency
    +   Promoting another region  has an RTO of < I minutes

Aurora ML 
-   Enables you to add ML-based prediction to your application via SQL
-   Simple , optimized, and secure integration between Aurora and AWS ML services
-   Supported services 
    +   Amazon Sage maker
    +   Amazon comprehend
-   You don't need to have ML experience 
-   Use cases : fraud detection , ads targeting , sentiment analysis
    product Recommendedations 

Amazon ElastiCache Overview
-   The same way RDS is to get managed Relational Database...
-   ElastiCache is to get managed Redis or Memcached 
-   Caches are in-memory databases with really high performance , low latency
-   Helps reduce load off of databases for read intensive workloads
-   Helps make your application stateless
-   AWS takes care of OS Maintenance / patching , optimizations , setup, config
    Monitoring , failure recovery and backups 
-   USING ELASTICACHE INVOLVES HEAVY APP CODE CHANGES 

ElastiCache SA - DB Cache 
-   Application qurey ElastiCache, if not available
    get from RDS and store in ElastiCache 
-   Helps relieve load in RDS
-   Cache must be an invalidation strategy to make sure only the most
    current data is used in there 

ElastiCache SA - User Session store
-   User logs into any of the application
-   The application writes the session data into ElastiCache
-   The user hits another instance of our application
-   The instance retrieves the data and the user is already logged in 

ElastiCache -  Redis vs Memcached
Redis : 
-   Multi AZ with Auto-failover 
-   Read Replicas to scale read and have high availability
-   Data Durability using AOF persistence
-   BACKUP AND RESTORE FEATURE 
Memcached : 
-   Multi-node for partitioning of data (sharding )
-   No high availability (replication)
-   Non persistent
-   No backup and restore 
-   Multi  threaded achitecture 

ElastiCache - Cache Security
-   All caches in ElastiCache
    +   DO NOT SUPPORT IAM AUTHEN
    +   Iam policies on ElastiCache are only useds for AWS API-level security
-   REDIS AUTH 
    +   You can set a "password/token" when you create a redis cluster
    +   This is an extra level of security for your cache 
    +   Support SSL in flight encryption
-   Memcached
    +   Supports SASL-based authen

Patterns for ElastiCache
-   LAZY LOADING  
-   WRITE THROUGH
-   SESSION STORAGE "
-   Quote:There are only two hard things in Computer Science:cache invalid & naming

ElastiCache - Redis UseCase 
-   Gaming Leaderboards are computationally complex
-   Redis Sorted sets gurantee both uniqueness and element ordering 
-   Each time a new element added, it's ranked in realtime, then add in correct order

Note for session 9 : LIST PORT Familiar
-   You should be able to differentitate between an important HTTPS - 443
    and Database port ( PostgreSQL  - 5432 )
-   IMPORTANT PORTS : 
    +   FTP : 21
    +   SSH : 22
    +   SFTP : 22 (same as SSH)
    +   HTTP : 80
    +   HTTPS : 443
    RDS DATABASE PORTS : 
    +   PostgreSQL : 5432
    +   MySQL : 3306
    +   Oracle RDS : 1521
    +   MariaDB : 3306
    +   Aurora: MySQL + PostgreSQL


Section 10 : Route 53
What is DNS
-   Domain name system which translates the human friendly hostnames into The
    machine IP address
-   www.google.com => 172.217.18.36
-   DNS is the backbone of the internet
-   DNS uses hierarchial naming structure 

DNS Terminologies
-   DOMAIN REGISTRAR 
-   DNS RECORDS
-   ZONE FILE 
-   NAME SERVER
-   TOP LEVEL DOMAIN (TLD)
-   SECOND LEVEL DOMAIN

Amazon Route 53
-   A highly available, scalable , fully managed and authortative DNS 
    +   Authortative  =the customer (you) can update the DNS records 
-   Route 53 is also a Domain REGISTRAR
-   Ability to check the health of your resuorces 
-   The only AWS service which provides 100%
-   Why route 53 ? 53 is a reference to the traditional DNS port 

Route 53 - Records 
-   Each record contains : 
    +   DOMAIN / SUBDOMAIN NAME 
    +   Record Type 
    +   Value 
    +   Routing policy
    +   TTL
-   Route 53 supports the following DNS record types 
    +   (must know ) A / AAAA / CNAME / NS 

Route 53 - Record Types 
-   A : maps a hostname to IPv4
-   AAAA : maps a hostname to IPv6
-   CNAME : maps a hostname to another hostname 
    +   The target is a domain name which must have an A or AAAA record 
    +   Can't crate a CNAME record for the top node of a DNS namespace (Zone Apex)
-   NS - Name Servers for the Hosted Zone 
    +   Control how traffic is routed for a domain  

Route 53 - Hosted Zones 
-   A container for records that define how to route traffic to a domain and sub it
    +   PUBLIC HOSTED ZONES 
    +   PRIVATE HOSTED ZONES
-   You pay $0.5/month with 1 hosted zone 

Route 53 - Records TTL (Time to Live)
-   High TTL - vd 24hr
    +   Less trafic on route 53
-   Low TTL - vd 60sec
    +   More traffic on route 53
    +   Records are outdated for less time 
    +   Easy to change records
-   EXCEPT FOR ALIAS RECORDS , TTL IS MANDATORY FOR EACH DNS RECORDS 
 
CNAME vs ALIAS
-   AWS Resources (Load Balancer , CloudFront...) expose an aws hostname :
-   CName : 
    +   Points a hostname to any other hostname
    +   ONLY FOR NON ROOT DOMAIN 
-   Alias : 
    +   Points a hostname to an AWS resource 
    +   Works for ROOT DOMAIN and NON ROOT DOMAIN
    +   Free of charge
    +   Native health check

Route 53 - Alias Records Targets 
-   Elastic Load Balancer
-   cloudFront Distribution
-   Api Gateway
-   Elastic Beanstalk environments
-   S3 Websites
-   VPC Interface Enpoints
-   Global Accelerator
-   Route 53 records in the same hosted zone 
-   YOU CANNOT SET AN ALLIAS RECORDS FOR AN EC2 DNS name 

Route 53 - Routing policies 
-   Define how Route 53 responds to DNS queries 
-   Don't get confused by the word "Routing "
    +   It's not the same as Load balancer routing which routes the traffic 
    +   DNS does not route any traffic , it only responds to the DNS queries
-   Route 53 Supports the following Routing policies 
    +   Simple 
    +   Weighted 
    +   Failover
    +   Latency based
    +   Geolocation
    +   Multi-value Answer
    +   Geoproximity (using Route 53 Traffic Flow feature)

Routing Policies - Simple
+   Typlically, route traffic to a single resource
+   Can specify multiple values in the same record
+   If multiple values values are returned, a random one is chosen by the client
+   When alias enabled , specify Only One AWS resource
+   Can't be associated with health checks

Routing Policies - Weighted
+   Control the % of the request that go to each specific resource
+   Assign each record a relative weight :
    Traffic (%) = Weight for a record / sum all weight for all records 
+   DNS Records MUST HAVE SAME NAME - TYPE
+   Can be associated with Health checks
+   Use cases : Load balancing between regions , testing new application versions
+   Assign a weight of 0 to a record to stop sending traffic to a resource
+   If all records have weight of 0, then all records will be returned equally 

Routing policies - Latency - based 
+   Redirect to the resource that has the least latency close to us 
+   Super helpful when latency for users is a priority
+   Latency is based on traffic between users and AWS Regions 
+   Germany users maybe directed to the US (if that's the lowest latency )
+   Can be associated with Health Checks (has a failover capability)

Route 53 - Health Checks 
-   HTTP health checks ONLY PUBLIC RESOURCES
-   Health Check => Automated DNS Failover
    1 : Health checks that monitors an enpoint 
    2 : Health checks that montior other health checks
    3 : Health checks that monitor Cloudwatch Alarms 
-   Health Checks are intergated with CW metrics

Health checks - Montior an Endpoint 
-   About 15 global health checker will check the enpoint health
    +   Healthy / unhealthy Threshold 
    +   Interval - 30s
    +   Supported protocol: HTTP,HTTPS and TCP 
    +   If > 18% of health checkers report the enpoint is healthy , Route 53
        considers it Healthy. Otherwise , it's unhealthy
    +   Ability to choose which locations you want Route 53 to use
-   Health Checks pass only when the endpoint responds with the 2xx and 3xx status
-   Health checks  pass/fail based on the text in the first 5120b / response
-   Config you router/firewall to allow incoming req from Route 53 health checks

Route 53 - Calculated Health checks
-   Combines the results of multiple Health checks into a single health check
-   You can use OR , AND , or NOT 
-   Can monitor up to 256 Child Health Checks
-   Specify how many of the health checks need to pass to make the parent pass
-   Usage:perform maintenance your website without causing all health check to fail

Health checks - Private hosted zones 
-   Route 53 health checkers are outside the VPC
-   They can't access private endpoints
-   You can create a CloudWatch METRIC and associated a CloudWatch ALARM,
    Then create a Health check that check the alarm itself

Route Policies - Geolocation
-   different from Latency-based!
-   THIS ROUTING IS BASED ON USER LOCATION 
-   Specify location by Continent , Country or by US State 
-   Should create a Default record 
-   Use cases: website localization, restrict content distribution, load balancing
-   Can be associated with Health Check

Geoproximity Routing Policy
-   Route traffic to your resources based on the geographic location of user
-   Ability TO SHIFT MORE TRAFFIC TO RESOURCES BASED on the defined BIAS
-   To change the size of the geographic region , specify BIAS values: 
    +   To expand (1 to 99) - more traffic to the resource
    +   To shrink (-1 to 99) - less traffic to the resource
-   Resources can be :
    +   AWS resources 
    +   Non-AWS Resources
-   You MUST USE ROUTE 53 TRAFFIC FLOW to use THIS FEATURE

Routing policies - Multi value 
-   Use when routing traffic to multiple resources 
-   Route 53 return multiple values / resources 
-   Can be associated with health checks 
-   Up to 8 healthy records are returned for each multi-value query
-   Multi value is not a substitue for having an ELB 

Domain Registar vs DNS service
-   You buy or register your domain name with a Domain Registar typlically by 
    paying annual charges (domain company )
-   The domain Registar provides you with a DNS service to manage your DNS record
-   But u can use another DNS service to manage your DNS records
-   E.g:Purchase the domain from Go Daddy and use Route 53  manage your DNS record

3rd Party Registar with amazon route 53
-   u buy your domain on 3rd party Registar,u use Route 53 as DNS service provider
    +   Create a Hosted zone in Route 53
    +   Update NS records on 3rd party website to use Route53 NAME SERVERS 
    +   DOMAIN REGISTRAR != DNS service
    +   But every Domain Registar usually comes with some DNS Features 

Keyword has remember
Section 11 : aws webflow pattern 
pattern 1 :
-   public vs Private IP and ec2
-   Elastic IP vs Route 53 vs Load balancer
-   Route 53 TTL , A records and alias records
-   Maintain ec2 intsance vs ASG (Auto scalling groups)
-   Multi AZ to survice disaster
-   ELB health checks
-   Security Group rules
-   Reservation of Capacity for costing savings when possible
-   considers 5 pillars for a well achitected application:
-   costs, performance , reliability , security , operational excellence
pattern 2 : 
-   ELB sticky session
-   Web client for storing cookies and making our web app stateless
-   ElastiCache
    +   For storing sessions 
    +   For caching data from RDS 
    +   Multi AZ
-   RDS
    +   For storing user data
    +   Read replica for scaling reads 
    +   Multi AZ for disater recovery
-   Tight Security with security group referencing each other 
pattern 3 : 
-   Aurora database to have easy multi az and read replica
-   Storing data in EBS (single instance app)
-   vs Storing data in EFS ( distribution app)


Instantiating app quickly
-   EC2 Instances : 
    +   USE GOLDEN AMI
    +   BOOTSTRAP USING USER DATA 
    +   HYBRID 
-   RDS databases : 
    +   Restore from a snapshot : the database will have schemas and data ready
-   EBS volumes:
    +   Restore from snapshot: the disk will already be formatted and have data!

Elastic Beanstalk - Overview
-   Elastic Beanstalk is a developer centric view of deploying application on AWS
-   It uses all the component's we've seen before : EC2, ASG , ELB, RDS , ...
-   Managed service
    +   Automatically handles capacity provisioning, load balancing, scaling
        application health monitoring , instance config ...
    +   Just the application code is the responsibility of the developer
-   We still have full control over the configuration
-   Beanstalk is free but you pay for the underlying instance

Elastic Beanstalk - components 
-   Application: collection of Elastic Beanstalk component
-   Application version : an iteration of your application code
-   Environments
    +   Collection of AWS resource running an application version 
    +   Tier : Web Server Environment Tier & Worker Environment Tier
    +   You can create multi environments

Elastic Beanstalk - support platforms : 
-   Go, JAVA SE, JAVA TOMCAT, net core linux, net win server,node, php, .py, ruby
-   Packer builder, single - multi container docker, pre config docker 
-   if not supported , you can write your custom platform (advanced )

Section 12 : Amazon s3 introduction 
Section introduction:
-   S3 is one of the main building block of AWS
-   it's advertised as " infinitely scaling " storage
-   it's widely popular and deserves its own section

Amazon s3 overview - Buckets
-   Amazon s3 allows people to store object (files) in "buckets" (directories)
-   Buckets must have a GLOBALLY UNIQUE NAME
-   Buckets are defined at the region level
-   Naming convention
    +   No uppercase
    +   No underscore
    +   3-63 character long 
    +   Not an IP 
    +   Must start with lowercase letter or number

Amazon S3 Overview - Objects
-   Objects (files) have a key
-   The key is the FULL path :
    +   s3://my-bucket/𝐦𝐲_𝐟𝐢𝐥𝐞.𝐭𝐱𝐭
    +   s3://my-bucket/𝐦𝐲_𝐟𝐨𝐥𝐝𝐞𝐫/𝐚𝐧𝐨𝐭𝐡𝐞𝐫_𝐟𝐨𝐥𝐝𝐞𝐫/𝐦𝐲_𝐟𝐢𝐥𝐞.𝐭𝐱𝐭
-   The key is composed of 𝗣𝗿𝗲𝗳𝗶𝘅 + 𝑜𝑏𝑗𝑒𝑐𝑡 𝑛𝑎𝑚𝑒
    +   s3://my-bucket/𝗺𝘆_𝗳𝗼𝗹𝗱𝗲𝗿/𝗮𝗻𝗼𝘁𝗵𝗲𝗿_𝗳𝗼𝗹𝗱𝗲𝗿/𝑚𝑦_𝑓𝑖𝑙𝑒.𝑡𝑥𝑡

-   There's no conecpt of "directories" within buckets 
-   Just keys with very long names that contain slashes
-   Objects values are the content of the body:
    +   Max Objects Size is 5TB (5000GB)
    +   If uploading more than 5GB , must use "multi-part upload"
-   Metadata (list of text key / value pairs - system of user metadata)
-   Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
-   Version ID (if versioning is enabled)

Amazon s3 - Versioning 

-   You can version your files in Amazon s3
-   It is enabled at the BUCKET LEVEL
-   Same key overwrite will increment the "version" : 1,2,3...
-   It is best practice to version your buckets 
    +   Protect against unintended deletes 
    +   Easy roll back to previous version
-   Notes : 
    +   Any File that is not ver prior to enabling versioning will've ver "null"
    +   Suspending versioning does not delete the previous versions


Encrypt s3 bucket data 
SSE - S3
-   SSE - S3 : encryption using keys handled & managed by amazon s3
-   Object is encrypted server side
-   AES - 256 encryption type 
-   Must set header : "x-amz-server-side-encryption":"AES256"

SSE - KMS
-   SSE - KMS : encryption using keys handled & managed by KMS
-   KMS Advantages : user control + audit trail
-   Object is encrypted server side
-   Must set header : "x-amz-server-side-encryption":"aws:kms"

SSE - C
-   SSE - C : server-side encryption using data keys fully managed by the
    customer outside of AWS 
-   Amazon S3 does not store the encryption key you provide
-   HTTPS must be used 
-   Encryption key must provided in HTTP headers , for every HTTP req made

Client side encryption
-   Client library such as the Amazon s3 Encryption Client
-   Client must encrypt data themselves before sending to S3
-   Client must decrypt data themselves when retrieving from S3
-   Customer fully manages the keys and encryption cycle

Encryption in transit (SSL/TLS )
-   Amazon s3 exposes : 
    +   HTTP endpoint : non encrypted
    +   HTTPS endpoint : encryption in flight 
-   You're free to use the endpoint you want, but HTTPS is Recommended
-   Most clients would use the HTTPS endpoint by default 
-   HTTPS is mandatory for SSE-C 
-   Encryption in flight is also called SSL/TLS 

S3 Security
-   USER BASED 
    +   Iam policies - Which Api call should be allow a specific user from IAM 
-   RESOURCE BASED 
    +   Bucket Policies - bucket wide rules from s3 console - allow cross account
    +   Object access control list (ACL) - finer grain
    +   Bucket Access Control List (ACL) - less common
-   Not : an IAM principal can access an S3 object if 
    +   the user IAM permissions allow it OR the resource policy ALLOW it
    +   AND there's no explict DENY

S3 bucket policies
-   JSON based policies
    +   Resource : bucket and object
    +   Actions : Set of API to Allow or Deny
    +   Effect: ALlow / Deny
    +   Principal : the account or user to apply the policy to 
-   Use S3 bucket for policy to : 
    +   Grant public access to the bucket 
    +   Force object to be encrypted at upload
    +   Grant access to another account 

Bucket settings for BLock Public Access 
-   block public access to buckets and objects granted through 
    +   new access control lists (ACLs)
    +   any access control lists (ACLs)
    +   new public bucket or access point policies 
-   Block public and cross account access to buckets and objects through any
    public bucket or access point policies
-   These settings were created to prevent company data leaks
-   If you know your bucket should be public , leave these on 
-   Can be set at the account level

S3 Security - Other 
-   Networking :
    +   Supports VPC endpoints 
-   Loggin and Audit :
    +   S3 Access logs can be stored in other S3 bucket 
    +   API calls can be logged in AWS CloudTrail
-   User Security
    +   MFA Delete : MFA can be required in versioned buckets to delete objects
    +   Pre-Signed URLs: URLs that are valid only for a limited time 

S3 Websites
-   S3 can host static websites and have them accessiable on the www
-   The website URL will be 
    +   <bucket-name>.s3-website-<AWS-region>.amazonaws.com
-   If u get a 403 (forbidden) err,make sure the bucket policy allows public reads 

CORS - Explained
-   An origin is a scheme (protocol) , host (domain) and port 
    +   E.g: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
-   CORS means Cross - Origin Resource Sharing
-   WEB BROWSER based mechanism allow request other origin while visit main origin
-   Same origin : h̵t̵t̵p̵:̵/̵/̵e̵x̵a̵m̵p̵l̵e̵.̵c̵o̵m̵/app1 & h̵t̵t̵p̵:̵/̵/̵e̵x̵a̵m̵p̵l̵e̵.̵c̵o̵m̵/app2
-   Different origin : h̵t̵t̵p̵:̵/̵/̵e̵x̵a̵m̵p̵l̵e̵1̵.̵c̵o̵m̵  & h̶t̶t̶p̶:̶/̶/̶o̶t̶h̶e̶r̶.̶e̶x̶a̶m̶p̶l̶e̶1̶.̶c̶o̶m̶
-   The request won't be fulfilled UNLESS the other origin allows for the 
    request , using CORS HEADERS ( vd : ACCESS-CONTROL-ALLOW-ORIGIN)

S3 CORS 
-   If a client does a cross-origin req on s3 bucket, we need enabled CORS header
-   It's a popular exam question
-   You can allow for a specific origin or for * (all origins , all method)

Amazon s3 - Consistency Model
-   Strong consistency as of December 2020
-   After a:
    +   Successful write of a new object (new PUT)
    +   or an overwrite or delete of an existing object (overwrite PUT or DELETE)
-   ...any:
    +   Subsequent read request immediately RECEIVES LATEST VERSION of the object
    +   Subsequent list request immediately reflects changes
-   Available at no additional cost without any performance impact

AWS EC2 Instance Metadata
-   AWS EC2 Instance Metadata is powerful but one of least known feat to dev
-   It Allows AWS EC2 instances to "learn about themselves" without using an 
    IAM ROles that purpose
-   The URL is 𝟭𝟲𝟵.𝟮𝟱𝟰.𝟭𝟲𝟵.𝟮𝟱𝟰/𝗹𝗮𝘁𝗲𝘀𝘁/𝗺𝗲𝘁𝗮-𝗱𝗮𝘁𝗮
-   u can retrieve IAM Role name from metadata, but u CAN"T retrieve IAM policy
-   Metadata = info about the EC2 instance
-   Userdata = launch script of the EC2 instance

AWS SDK Overview 
-   What if u want perform actions on AWS directly from your app code (use CLI)
-   U can use an SDK
-   Offical SDKs are ...
    +   JAVA,Net,Node,PHP,PYTHON (named boto3 / botocore),GO,RUBY,C++ 
-   We have to use AWS SDK when coding against AWS Services such as DynamoDB
-   Fun fact... the AWS CLI uses the PYTHON SDK (BOTO3)
-   The exam expects you to know when you shoule use an SDK
-   We will preactice the AWS SDK when we get to LAMBDA FUNCTION
-   Good to know: if u don't specify or config default region => us-east-1

Section 14: S3 MFA- Delete 
-   MFA (multi factor authentication) forces user to generate a code on a device
    (usually a mobile phone or hardware) before doing important operations on S3
-   To use MFA - Delete , enabled versioning on the s3 bucket
-   You will need MFA to 
    +   permanently delete an object version
    +   suspend versioning on the bucket
-   You won't need MFA for 
    +   enabling versioning 
    +   listing deleted versions 
-   ONLY THE BUCKET OWNER (ACC ROOT) can enable/disable MFA-Delete
-   MFA - Delete currently can only be enabled using the CLI
 
 S3 Default Encryption vs Bucket Policy
-   One way to "force encryption" is to use a bucket policy and refuse any API
    call to PUT an S3 object without encryption header
-   Another way is to use the "default encryption " option in s3
-   Note : Bucket Policies are evaluated before default encryption

S3 Access Logs 
-   For audit purpose , you may want to log all access to s3 buckets 
-   Any request made to S3 , from any account , authorzied or denied
    will be logged into another s3 bucket 
-   That data can be analyzed using data analysis tools ...
-   Or amazon athena as we'll see later in this section 
-   The log format is at : 
    𝗵𝘁𝘁𝗽𝘀://𝗱𝗼𝗰𝘀.𝗮𝘄𝘀.𝗮𝗺𝗮𝘇𝗼𝗻.𝗰𝗼𝗺/𝗔𝗺𝗮𝘇𝗼𝗻𝗦𝟯/𝗹𝗮𝘁𝗲𝘀𝘁/𝗱𝗲𝘃/𝗟𝗼𝗴𝗙𝗼𝗿𝗺𝗮𝘁.𝗵𝘁𝗺𝗹

S3 Access Logs : Warning 
-   Do not set your logging bucket to be the montiored bucket 
-   It will create a logging loop, and 𝘆𝗼𝘂𝗿 𝗯𝘂𝗰𝗸𝗲𝘁 𝘄𝗶𝗹𝗹 𝗴𝗿𝗼𝘄 𝗶𝗻 𝘀𝗶𝘇𝗲 𝗲𝘅𝗽𝗼𝗻𝗲𝗻𝘁𝗶𝗮𝗹𝗹𝘆

S3 Replication (CRR - SRR)
-   𝗠𝘂𝘀𝘁 𝗲𝗻𝗮𝗯𝗹𝗲 𝘃𝗲𝗿𝗶𝘀𝗼𝗻𝗶𝗻𝗴 𝗶𝗻 𝘀𝗼𝘂𝗿𝗰𝗲 𝗮𝗻𝗱 𝗱𝗲𝘀𝘁𝗶𝗻𝗮𝘁𝗶𝗼𝗻 
-   Cross Region Replication ( CRR )
-   Same Region Replication ( SRR )
-   Buckets can be in different accounts 
-   Copying is ASYNC
-   Must give proper IAM permisson to S3 
-   𝘾𝙍𝙍 - 𝙐𝙨𝙚 𝙘𝙖𝙨𝙚𝙨 : compliance, lower latency access, replication across acc
-   𝙎𝙍𝙍 - 𝙐𝙨𝙚 𝙘𝙖𝙨𝙚𝙨 : log aggregation , live replication between
    production and test account 

S3 Replication - Notes 
-   After activating , only new objects are replicated 
-   Optionally, you can replicate existing objects using S3 Batch Replication
    +   Replicates existing object and objects that failed replication 
-   For DELETE operations : 
    +   Can replicate delete markers from source to target ( optional setting )
    +   Deletions with a version ID are not replicated (avoid malicious deletes)
-   There is no " chaning " of replication 
    +   if bucket I has replication into bucket 2, which has replication into b3 
    +   then object created in bucket are not replication to b3 

S3 pre-signed URL
-   Can generated using SDK or CLI 
-   For downloads ( CLI )
-   For uploads ( must using SDK )
-   Valid for default 3600s , can change timeout with --expries-in TIME_BY_S arg 
-   Users given pre-signed URL inherit permission of persion who Generate URL
    for GET - PUT 
-   Examples : 
    +   Allow only logged in - users to download previum video on your s3 bucket
    +   Allow an ever changing list of users to download files by gen URL dynamic
    +   Allow temp a user upload a file to precise loaction on your bucket 

S3 Storage Classes 
-   Amazon S3 Standard - General Purpose 
-   Amazon s3 Standard - Infrequent Access 
-   Amazon s3 One Zone infrequent Access 
-   Amazon s3 Glacier Instant Retrieval
-   Amazon s3 Glacial Flexible Retrieval
-   Amazon s3 Glacial Deep Archive
-   Amazon s3 interlligent Tiering 
-   Can more between class manually or using s3 LIFECYCLE CONFIG

S3 Durability and Avalability
-   Durability:
    +   High durability (99.99% || 9's) of object accross multi AZ 
    +   If you store 10 milion object with S3 , u can average expect to incur
        a loss of single object ones every 10k years 
    +   Same for all storage class
-   Avalability
    +   Measures how readily Available a service is 
    +   Varies depending on storage class 
    +   Ex : S3 Standard has 99.99% availability = not 53 minutes / year (err) 

S3 Standard - General Purpose 
-   99.99% availability
-   User for requently access data
-   low latency and high thoughput 
-   Sustain 2 concurrent failure 
-   UseCase : Big Data anylitics , mobile - gaming app , content distribution...

S3 Storage access - Infrequent access 
-   For data that is less frequently required  ,but requires access when need 
-   Lower cost than S3 Standard

-   𝗔𝗺𝗮𝘇𝗼𝗻 𝘀𝟯 𝗦𝘁𝗮𝗻𝗱𝗮𝗿𝗱 𝗶𝗻𝗳𝗿𝗲𝗾𝘂𝗲𝗻𝘁 𝗮𝗰𝗰𝗲𝘀𝘀 (𝗦𝟯 𝗦𝘁𝗮𝗻𝗱𝗮𝗿𝗱 -𝗜𝗔)
    +   99.9% availability
    +   Use Cases : Disaster Recovery, backups 
-   𝐀𝐦𝐚𝐳𝐨𝐧 𝐬𝟑 𝐎𝐧𝐞 𝐙𝐨𝐧𝐞 𝐈𝐧𝐟𝐫𝐞𝐪𝐮𝐞𝐧𝐭 𝐀𝐜𝐜𝐞𝐬𝐬 ( 𝐎𝐧𝐞 𝐙𝐨𝐧𝐞 - 𝐈𝐀)
    +   High durability (99.99%) in a single AZ ; data lost when AZ is destroyed
    +   99.95 availability
    +   Use cases : Story secnondary backup copies of on - premise data 
        or data you can recreate  
-   Amazon s3 Glacier Storage Classes 
    +   Low - cost object storage meant for archiving / backup
    +   Pricing : price for storage + object retrieval cost 
-   Amazon S3 Glacier Instant Retrieval 
    +   Milisecond retrieval , great for data accessed once a quarter
    +   Minium storage duration of 90 days
-   Amazon S3 Glacier Flexible Retrieval (formerly Amazon s3 Glacier)
    +   Expedited (1 to 5 min), Standard (3 to 5 hr),Bulk (5 to 12hr) - free
    +   Minium storage duration of 90 days
-   Amazon S3 Glacier Deep Archive - for long term to storage 
    +   Standard ( 12hr ) , Bulk (48hr )
    +   Minium storage duration of 180 days

S3 Interlligent - Tiering 
-   Small monthly montioring and auto - tiering fee 
-   Moves objects automatically between Access Tier based on usuage
-   There are no retrieval charges in S3 Interligent - Tiering 
-   Frequent Access tier (automatic): default tier 
-   Infrequent Access tier (automatic): objects not accessed for 30days 
-   Archive Instant Access Tier(automatic) : objects not accessed for 90days 
-   Archive Access tier : 90 > 700+ days
-   Deep Archive Access tier : 120 > 700+ days

S3 Lifecycle rules
-   𝗧𝗿𝗮𝗻𝘀𝗶𝘁𝗶𝗼𝗻 𝗮𝗰𝘁𝗶𝗼𝗻𝘀: if defines when object're transitioned another storage class
    +   Move objects to Standard IA class 60 days after creation 
    +   Move to Glacier for archiving after 6 month
-   𝗘𝘅𝗽𝗶𝗿𝗮𝘁𝗶𝗼𝗻 𝗮𝗰𝘁𝗶𝗼𝗻𝘀 : configure object to expire (delete) after some time 
    +   access log file can be delete when out of 365 days 
    +   𝗰𝗮𝗻 𝗯𝗲 𝘂𝘀𝗲𝗱 𝘁𝗼 𝗱𝗲𝗹𝗲𝘁𝗲 𝗼𝗹𝗱 𝘃𝗲𝗿𝘀𝗶𝗼𝗻 𝗼𝗳 𝗳𝗶𝗹𝗲 (𝗶𝗳 𝘃𝗲𝗿𝘀𝗶𝗼𝗻𝗻𝗶𝗻𝗴 𝗶𝘀 𝗲𝗻𝗮𝗯𝗹𝗲𝗱 )
    +   Can be used to delete incomplete multi-part uploads
-   Rules can be created for a certain prefix (ex - S3://mybucket/mp3/*)
-   Rules can be created for certain objects tags (ex - Department : Finance)

S3 Lifecycle Rules - Scenario I
-   Your application on EC2 creates images thumbnails after profile photos are 
    Uploaded to Amazon s3 . These thumbnails can be easilly created , and only
    need to be kept for 45 days . The source images should be able to be
    immediately retrieved for these 45 days, and afterwards , the user can wait
    up to 6 hr . How would u design this ? 
-   Answer :
    +   S3 source images can be on STANDARD , with a lifecycle config transition
        them to GLACIER after 45 days 
    +   S3 thumbnails can be on ONEZONE_IA with a lifecycle config expire them
        after 45 days 
S3 Lifecycle Rules - Scenario 2
-   A rule in your company states that you sould be able to recover your deleted
    S3 objects immediately for 15 days, although this may appen rarely.After this
    time , And for up to 1 years, deleted object sould be recoveable with in 48 hr
    Solution : 
    +   You enabled s3 versioning in order to have object versions, so that 
        "delete objects" are in fact hidden by "delete marker" and can be recovered
    +   You can transition these "noncurrent versions" of the object to S3_IA
    +   You can transition afterwards these "noncurrent versions" to DEEP_ARCHIVE

S3 Analytics - Storage Class analysis
-   You can setup S3 analytics to help determine when to stransition objects from 
    Standard to Standard_IA
-   Does not work for ONEZONE_IA or GLACIER 
-   Report is updated daily 
-   Takes about 24h to 48hr to first start 
-   Good first step to put together Lifecycle Rules (or improve them)!

S3 - Baseline Performance 
-   Amazon s3 automatically scales to high request rates, latency 100-200ms 
-   Your application can archieve at least 3,500 PUT/COPY/POST/DELETE and
    5,500 GET/HEAD requests per second per prefix in a bucket
-   There are no limits to the number of prefixes in a bucket 
-   Example (Object path => prefix)
    +   bucket/folder1/sub1/file => folder1/sub1
-   If you spread reads across all four prefixes evenly, you can achieve 22,000
    requests per second for GET and HEAD 

S3 - KMS Limitation 
-   If you use SSE - KMS, you may be impacted by the KMS limits
-   When u upload, it calls the 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗲𝗗𝗮𝘁𝗮𝗞𝗲𝘆 KMS API 
-   When u download, it calls the 𝗗𝗲𝗰𝗿𝘆𝗽𝘁 KMS API 
-   Count towards the KMS quota per s (5500, 10k, 30k req/s on the region )
-   you can request a quota using Service Quota Console 

S3 Performance 
-   Multi-part upload 
    +   Recommended for files > 100MB, must use for files > 5GB
    +   Can help parallelize uploads (speed up transfers)
-   S3 transfer Acceleration 
    +   Increase transfer speed by transferring file to an AWS edge location which
        will forward the data to the s3 bucket in the target region
    +   Compatible with multi-part upload 

S3 Performance - S3 Byte -Range fetches 
-   parallelize gets by requesting specific byte ranges 
-   better resilience in case of failures 
-   CAN BE USED TO SPEED UP DOWNLOAD

S3 Select & Glacier Select 
-   Retrieve less data using SQL by performming server side filtering 
-   can filter by col & row ( simple SQL statements )
-   less network transfer , less cpu cost client side 

s3 Event Notification
-   s3:ObjectCreated , s3:ObjectRemoved , s3: ObjectRestore, s3:Replication
-   Object name filtering possible (*.jpg)
-   Usecase : generate thumbnauls of images uploaded to s3 
-   𝗖𝗮𝗻 𝗰𝗿𝗲𝗮𝘁𝗲 𝗮𝘀 𝗺𝗮𝗻𝘆 "𝘀𝟯 𝗲𝘃𝗲𝗻𝘁𝘀" 𝗮𝘀 𝗱𝗲𝘀𝗶𝗿𝗲𝗱 
-   s3 event notification typlically deliver events in seconds but can 
    sometimes take a minite of longer 

s3 Event notifications with amazon EventBridge
-   Advanced filtering option with JSON rules 
-   multiple distinations
-   EventBridge Capacibilities

s3 Requester Pays 
-   In general , bucket owners pay for all Amazon s3 storage and data transfer
    const associated with thieir bucket
-   With requester Pays buckets, the requester instead of the bucket owner
    pay the const of req and the data download from the bucket 
-   helpful when u want to share large datasets with author account 
-   the quester muse e a auth 


Amazon athena 
-   SERVERLESS  query service to PERFORM ANALYTICS AGAINST S3 OBJECTS 
-   Uses standard SQL language to query the files 
-   Supports CSV, JSON, ORC, AVRO, and Parquet (built on Presto)
-   Pricing: $5.00 per TB of data scanned 
-   Use compressed or columnar data for cost savings (less scan)
-   Use cases: Business intelligence / analytics / reporting ,...

NOTE : ANALYZE DATA IN S3 USING SERVERLESS SQL , USE ATHENA

S3 Object Lock (versioning must be enabled)
-   Adopt a WORM (Write Once Read Many) model
-   Block an object version deletion for a specificd amout of time
-   Object retention:
    +   Retention Period : specifies a fixed period
    +   Legal Hold: same protection, no expiry date
-   Modes:
    +   Governance mode: users can't overwrite or delete an object version or
        alter its lock settings unless they have special permissions
    +   Compliance mode:a protected object version can't be overwritten or deleted
        by any user, including the root user in your AWS account. When an object
        is locked in compliance mode, its retention mode can't be changed, and its
        retention period can't be shortened

SECTION 15 : 
AWS Cloud Front 
-   (CDN) Content Delivery Network
-   Improves read performance, content is cached at the edge
-   216 Point of Presence globally (edge locations)
-   DDoS protection, integration with Shield, AWS Webapp firewall
-   can expose https and can talk to internal HTTPS backends 

CloudFront - Origins 
-   S3 bucket 
    +   for distributing file and caching them at the edge
    +   Enhanced security with Cloudfront OAI ( Origin Access Identity )
    +   Cloudfront canbe used as an as ingress (to upload files to s3 )
-   Custom origin(HTTP)
    +   ALB (app load balancer)
    +   ec2 instance 
    +   s3 website 
    +   Any HTTP backend u want 

CloudFront Geo Restriction
-   u can restrict who can access your distribution
    +   whitelist 
    +   blacklist 
-   The "country" is determined using a 3rd party Geo-IP database 

Cloudfront vs s3 cross region replication
-   CloudFront :
    +   Global edge network
    +   file are cached by TTL (maybe a day)
    +   𝗚𝗿𝗲𝗮𝘁 𝗳𝗼𝗿 𝘀𝘁𝗮𝘁𝗶𝗰 𝗰𝗼𝗻𝘁𝗲𝗻𝘁 𝘁𝗵𝗮𝘁 𝗺𝘂𝘀𝘁 𝗯𝗲 𝗮𝘃𝗮𝗶𝗹𝗮𝗯𝗹𝗲 𝗲𝘃𝗲𝗿𝘆𝘄𝗵𝗲𝗿𝗲
-   S3 Cross Region Replication : 
    +   Must be setup for each region you want replication to happens
    +   Files are updated in near real-time 
    +   Read only
    +   𝗴𝗿𝗲𝗮𝘁 𝗳𝗼𝗿 𝗱𝘆𝗻𝗮𝗺𝗶𝗰 𝗰𝗼𝗻𝘁𝗲𝗻𝘁 𝘁𝗵𝗮𝘁 𝗻𝗲𝗲𝗱𝘀 𝘁𝗼 𝗯𝗲 𝗮𝘃𝗮𝗶𝗹𝗮𝗯𝗹𝗲 𝗮𝘁 𝗹𝗼𝘄-𝗹𝗮𝘁𝗲𝗻𝗰𝘆 𝗶𝗻 𝗳𝗲𝘄 𝗿𝗲𝗴𝗶𝗼𝗻𝘀

CloudFront Signed URL / Signed Cookies 

-   You want to distribute paid shared content to premium user over the world 
-   we can use CloudFront sign URL / Cookie . We attach a policy with: 
    +   Includes URL expiration
    +   include ip range to access data from 
    +   trusted signers 
-   how long should url be valid for ? 
    +   share content : make it short 
    +   private content : you can make it last for years 
-   Signed url = access to individual files 
-   signed cookies = access to multiple files 

CloudFront Signed URL vs S3 pre-signed URL 
-   Cloudfront Signed URL :
    +   Allow access to a path, no matter the origin
    +   Account wide key-pair , only the root can manage it 
    +   Can filter by ip, path , date, expiration
    +   can leverage caching features 
-   s3 pre-signed url 
    +   Issue a request as the person who pre-signed the URL
    +   Uses the IAM key
    +   limited lifetime 

CloudFront - Price Classes 
-   you can reduce the number of edge location for COST REDUCTION
-   Three price classes 
    +   100  :  only the least exprensive regions 
    +   200  :  most regions , but excludes the most exprensive regions 
    +   all  :  all regions -> best performance

CloudFront - multiple Origin 
-   To route to different kind of origins based on the content type 
-   Based on path pattern 
    +   /images/*

CloudFront - Origin Groups 
-   To increase high - availability and do failover
-   origin group : one primary and one  secnondary origin
-   if the primary origin fail , use second origin

CloudFront - Field Level Encryption
-   protect user sensitive infor thorugh app stack
-   adds an additional layer of security along with https 
-   sensitive information encrypted at the edge close to user 
-   uses asymmetric encryption
-   Usage : 
    +   Specify set of fields in POST request that u want to be encrypted 
    +   specify to public key to encrypted them

Global users for our application 
-   You have deployed an application and have global user who want access
-   They go over public internet, which can add litte latency due many hops
-   We wish to go as fast as possible through AWS network minimize latency

Unicast IP and Anycast IP 
-   Unicast IP : one server holds one ip address
-   Anycast IP : all servers holds the same ip client routes nearest one 

AWS Global Accelerator 
-   Leverage the AWS internal network to route to your application
-   2 Anycast IP create for your application
-   The Anycast IP send traffic directly to Edge Locations 
-   The Edge locations send the traffic to your application

AWS Global Accelerator 
-   Works with 𝗘𝗹𝗮𝘀𝘁𝗶𝗰 𝗜𝗣 , 𝗘𝗖𝟮 𝗶𝗻𝘀𝘁𝗮𝗻𝗰𝗲𝘀 , 𝗔𝗟𝗕 , 𝗡𝗟𝗕, 𝗽𝘂𝗯𝗹𝗶𝗰 𝗼𝗿 𝗽𝗿𝗶𝘃𝗮𝘁𝗲 
-   Consistent performance
    +   intelligent routing to lowest latency and fast regional failover 
    +   no issues with client cache 
    +   Internal aws network
-   Health Checks 
    +   Global Accelerator perform a health check of your application
    +   Helps make your application global
    +   Great for Disaster recovery
-   Security
    +   only 2 external IP need to be whitelisted 
    +   DDOS protection thanks to AWS shield 

AWS Global Accelerator vs CloudFront 
-   They both use AWS GLOBAL NETWORK and its edge location arround the world 
-   Both services intergrate with AWS Shield for DDOS protect

-   CloudFront :    
    +   Improves performance for both cacheable content 
    +   Content is served at the edge 
-   Global Accelerator
    +   Improves performance for a wide range for application over TCP or UDP
    +   Proxying packets at the edge to app running in 1 of more aws regions 
    +   Good fit for non-HTTP use cases, such as gaming (UDP),IOT,Voice over IP
    +   Good for HTTP use case that requies static ip address 
    +   Good for HTTP use case that required deterministic fast regional failover

AWS Snow Family
-   Highly - secure , portable devices to COLLECT AND PROCESS DATA AT THE EDGE
    and MIGRATE DATA INTO AND OUT OF AWS 
-   Data migration : snowcone , snowball edge , snowmobile 
-   Edge computing : snowcone , snowball edge
-   AWS Snow family : offline devices to perform data migrations 
    if it takes more than a week to transfer over network, use Snowball devices

Snowball Edge (for data transfers)
-   Physical data transport solution : move TBs or PBs of data in or out of AWS
-   Alternative to moving data over the network 
-   Pay per data transfer job 
-   Provide block storage and s3-compatible object storage 
-   𝗦𝗻𝗼𝘄𝗯𝗮𝗹𝗹 𝗲𝗱𝗴𝗲 𝗦𝘁𝗼𝗿𝗮𝗴𝗲 𝗢𝗽𝘁𝗶𝗺𝗶𝘇𝗲𝗱
    +   80𝘛𝘉 𝘰𝘧 𝘏𝘋𝘋 𝘤𝘢𝘱𝘢𝘤𝘪𝘵𝘺 for block volume and s3 compatible object storage 
-   𝗦𝗻𝗼𝘄𝗯𝗮𝗹𝗹 𝗲𝗱𝗴𝗲 𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗢𝗽𝘁𝗶𝗺𝗶𝘇𝗲𝗱
    +   42𝘛𝘉 𝘰𝘧 𝘏𝘋𝘋 𝘤𝘢𝘱𝘢𝘤𝘪𝘵𝘺
-   Use cases : Large data cloud migrations, DC decommission, disaster recovery

AWS Snow Cone 
-   𝗦𝗺𝗮𝗹𝗹, 𝗽𝗼𝗿𝘁𝗮𝗯𝗹𝗲 𝗰𝗼𝗺𝗽𝘂𝘁𝗶𝗻𝗴,𝗮𝗻𝘆𝘄𝗵𝗲𝗿𝗲,𝗿𝘂𝗴𝗴𝗲𝗱 & 𝘀𝗲𝗰𝘂𝗿𝗲,𝘄𝗶𝘁𝗵𝘀𝘁𝗮𝗻𝗱𝘀 𝗵𝗮𝗿𝘀𝗵 𝗲𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁𝘀
-   Light (4.5 pounds , 2.1 kg)
-   8 TB mem of usable storage 
-   Device use for edge computing , storage , and data transfer 
-   use Snowcone where snowball does not fit 
-   Must provide your own battery / cables
-   Can be sent back to AWS offline, or connect it to internet and use
    AWS DATASYNC to send data 

AWS Snowmobile 
-   Transfer exabytes of data 
-   Each Snowmobile has 100PB of capacity 
-   High security: temperature controlled, GPS, 24/7 video surveillance
-   BETTER THAN SNOWBALL IF U TRANSFER MORE THAN 10 PBs

Snow Family - Usage Process 
1.  Request Snowball devices from AWS console for Delivery
2.  Install the snowball client / AWS obs hub on your servers 
3.  Connect the snowball to your servers and copy files using the client
4.  Ship back the device when you're done 
5.  Data will be loaded into s3 bucket 
6.  Snowball is completely wiped 

What is Edge Computing ? 
-   Process data while it's being created on EDGE LOCATION 
    +   A truck on the road, a ship on the sea, a mining station underground 

-   These location may have 
    +   Limited  / no internet access 
    +   limited / no easy access to computing power
-   We setup a 𝗦𝗻𝗼𝘄𝗰𝗼𝗻𝗲 / 𝗦𝗻𝗼𝘄𝗯𝗮𝗹𝗹 𝗘𝗱𝗴𝗲 device to do edge computing 
-   use cases of edge computing 
    +   preprocess data 
    +   Machine learning at the edge 
    +   Transcoding media streams 
-   Eventually we can ship back device to AWS 

SnowFamily - Edge Computing 
-   SnowCone  (smaller )
    +   2 CPUs , 4gb of mem , wire or wireless access 
    +   USB-c Power using a cord or optional battery 
-   Snowball Edge  - COMPUTE OPTIMIZED 
    +   52 𝘷𝘊𝘗𝘜𝘴 , 208 𝘎𝘪𝘣 𝘰𝘧 𝘙𝘈𝘔
    +   OPTIONAL GPU 
    +   42 TB storage usable
-   Snowball edge - STORAGE OPTIMIZED 
    +   Up to 40 vCPUs, 80 Gib of RAM 
    +   Object storage clustering avaliable 
-   All : Can run ec2 instance - lambda function
-   Long - term deployment options : 1 and 3 years discounted pricing 

AWS OPSHub 
-   Historically , to use Snow Family devices, your need CLI
-   Today , u can use AWS OPSHub to manage your SnowFamily device
    +   Unlocking and config single or clustered devices 
    +   Transfering files 
    +   Launching and managing instances running on Snow Family devices
    +   Monitor device metrics
    +   Launch compatible AWS services on your devices 

Solition Architecture : Snowball into glacier 
-   Snowball cannot import to glacier directly
-   You must use S3 first , in combination with an s3 LIFECYCLE POLICY 

    SNOWBALL => S3 => GLACIER

Amazon SFX - overview 
-   Launch 3rd party high-performance file system on aws 
-   Fully managed service 

Amazon SFX for windows (File Server)
-   EFS is a shared POSIX system for linux system 
-   FSX for Windows is a fully managed Windows file system share drive 
-   Supports SMB protocol & windows NTFS 
-   Microsoft Active Directory intergration , ACLs , user Quotas 
-   CAN BE MOUNTED ON LINUX / EC2 INSTANCES 

-   Scale up to 10s of GB/s, million of IOPS, 100s PB of data
-   Storage options :
    +   SSD - Latency sensitive workloads
    +   HDD - broad spectrum of workloads 
-   Can be accessed from your on-premise infrastructure
-   Can be config to be MULTI-AZ 
-   Data is backup daily to s3 

Amazon FSx for Lustre 
-   Lustre is a type of parallel distributed file system
    for large - scale computing 
-   The name Lustre is devired from "linux " and cluster 
-   Machine Learning , (HPC) High Performance Computing 
-   Video processing , Financial Modeling , Electronic Design Automation 
-   Scales up to 100s GB/s, millions of IOPS, sub-ms latencies 
-   Storage Options :
    +   SSD - Low latency , IOPS intensive workloads, small & random file
    +   HDD - Throughput - intensive workloads, large & sequential file 
-   SCAMLESS INTERGRATION WITH s3
    +   Can "Read s3" as a file system (through FSx)
    +   Can write the output of the computation back to s3 
-   Can be used from on-premise server 

FSx File System Deployment Options 
-   Scratch File System
    +   Temporary storage 
    +   Data is not replicated 
    +   High burst (6x faster , 200MBps per Tib)
    +   Usage : Short -term processing , optimize costs 
-   Persistent File system 
    +   Long - term storage 
    +   Data is replicated within same AZ 
    +   Replace failed files within minutes 
    +   Usage : Long-term  processing , sensitive data 

Hybrid Cloud for storage 
-   AWS is pushing for "Hybrid cloud "
    +   Part of your infrastructureis on the cloud 
    +   Part of your infrastructureis on premise
-   This can be due to 
    +   Long cloud migrations 
    +   Security requirements 
    +   Complice requirements 
    +   IT strategy
-   S3 is a proprietary storage technology (unlike NFS/EFS)
-   AWS Storage Gateway

AWS Storage Gateway
-   Bridge between on-permise data and cloud data in s3
-   Use cases: disaster recovery, backup & restore, tiered storage
-   3 types of storage gateway
    +   File gateway
    +   Volume gateway
    +   Tape gateway

-   Exam Tip : you need to know the difference all 3 type 

FILE GATEWAY 
-   Configured s3 bucket are accessiable using NFS AND SMB protocol
-   Supports s3 standards , s3 IA , s3 ONEZONE_IA
-   Bucket access using IAM roles for each File gateway
-   most recently used data is cached in the file gateway 
-   Can be mounted an many servers 
-   𝗜𝗻𝘁𝗲𝗿𝗴𝗿𝗮𝘁𝗲𝗱 𝘄𝗶𝘁𝗵 𝗔𝗰𝘁𝗶𝘃𝗲 𝗗𝗶𝗿𝗲𝗰𝘁𝗼𝗿𝘆 (𝗔𝗗) for user authen

VOLUME GATEWAY 
-   Block storage is ISCI protocol backed by s3 
-   Backed by EBS snapshots which can help restore premise volume
-   CACHE VOLUMES:low latency to most recent data 
-   Stored Volumes:entired dataset is on premise,sheduled backup to s3

TAPE GATEWAY 
-   Some companies have backup processes using physical tapes 
-   With tape gateway, companies use the same precess but, in the cloud 
-   (VTL) Virtual Tape Library , backed by s3 and glacier 
-   Backup data using existing tape-based process 
-   work with leading backup software vendors 

Storage Gateway - Hardware appliance 
-   Using Storage Gateway means you need on permises virtualization
-   Otherwise, you can use a 𝗦𝘁𝗼𝗿𝗮𝗴𝗲 𝗚𝗮𝘁𝗲𝘄𝗮𝘆 𝗛𝗮𝗿𝗱𝘄𝗮𝗿𝗲 𝗔𝗽𝗽𝗹𝗶𝗮𝗻𝗰𝗲
-   you can buy it an amazon.com
-   Work with File Gateway, Volume Gateway, Tape Gateway
-   Has a require CPU, mem,net ,SSD cache 
-   Helpful for daily NFS backup in small data centers 

AWS Storage gateway summary 
Examtip : READ THE QUESTION WELL , IT WILL HINT AT WHICH GATEWAY TO USE
-   On-permise data to the clooud => storage gateway
-   File access / NFS => File gateway
-   Volumes / blockStorage / isCsi => Tape gateway
-   No on premises virtualization => hardware Appliance 

Amazon FSx File Gateway 
-   Native access to Amazon FSx for Windows File Server
-   LOCAL CACHE FOR FREQUENTLY ACCESSES DATA 
-   Windows native compatibility
-   useful for group file share and home directories 

AWS Transfer Family 
-   A fully - managed service for file transfers into and out of Amazon
    S3 or Amazon EFS using the FTP protocol
-   Supported Protocols 
    +   AWS transfer for FTP 
    +   AWS transfre for FTPs 
    +   AWS transfre for SFTP
-   Managed infrastructure, Scalable,Reliable, Highly Available (multi-AZ)
-   Pay per provisioned endpoint per hour + data transfer in GB
-   Store and manage users crediental within the service
-   Intergrate with existing authen system
-   Usage : sharing files, public dataset CRP, ERP

Storage Comparison
-   𝗦𝟯 : Object storage 
-   𝗚𝗹𝗮𝗰𝗶𝗲𝗿 : Object Archival
-   𝗘𝗙𝗦 : Network file system for Linux instance , Posix filesystem
-   𝗙𝗦𝘅 𝗳𝗼𝗿 𝗪𝗶𝗻𝗱𝗼𝘄𝘀 : Network file system for windows servers 
-   𝗙𝗦𝘅 𝗳𝗼𝗿 𝗟𝘂𝘀𝘁𝗿𝗲: high performance computing linux file System
-   𝗘𝗯𝘀 𝘃𝗼𝗹𝘂𝗺𝗲𝘀 : network storage for one ec2 instance at a time 
-   𝗜𝗻𝘀𝘁𝗮𝗻𝗰𝗲 𝗦𝘁𝗼𝗿𝗮𝗴𝗲: Physical storage for your Ec2 instance (high IOPS)
-   𝗦𝘁𝗼𝗿𝗮𝗴𝗲 𝗚𝗮𝘁𝗲𝘄𝗮𝘆: File gateway , volume gateway, tape gateway 
-   𝗦𝗻𝗼𝘄𝗯𝗮𝗹𝗹 / 𝗦𝗻𝗼𝘄𝗺𝗼𝗯𝗶𝗹𝗲: to move large amout of data to the cloud, physically
-   𝗗𝗮𝘁𝗮𝗯𝗮𝘀𝗲: for specific workloads, usually with indexing and querying

SECTION 19: 
What's Serverless
-   Serverless is a new paradigm in which the developer don't have to manage
-   They just deploycode 
-   They just deploy ... functions ! 
-   Initially... Serverless == FaaS
-   Serverless was pioneered by AWS lambda but now also includes anything
    that's managed : "databases, messaging , storage , etc "
-   𝗦𝗲𝗿𝘃𝗲𝗿𝗹𝗲𝘀𝘀 𝗱𝗼𝗲𝘀 𝗻𝗼𝘁 𝗺𝗲𝗮𝗻 𝘁𝗵𝗲𝗿𝗲 𝗮𝗿𝗲 𝗻𝗼 𝘀𝗲𝗿𝘃𝗲𝗿𝘀 
    it means you just don't manage / provision / see them

Serverless in AWS 
-   AWS Lambda
-   Dynamo DB
-   AWS Coginto
-   AWS API GATEWAY
-   Amazon s3
-   AWS SNS & SQS 
-   AWS Kinesis Data firehose 
-   Aurora serverless 
-   Step functions 
-   Fargate 

Why AWS lambda 
-   EC2 :
    +   Virtual server on the Cloud 
    +   Limited by ram and cpu
    +   continuously running 
    +   Scaling means intervnetion to add/remove servers 
-   Lambda :
    +   Virtual functions - no server to manage 
    +   Limited by time ( 15min)
    +   Run on demand 
    +   Scaling is automated 

Benefits of AWS Lambda 
-   Easy pricing 
    +   Pay per request and compute time 
    +   Free tier of 1 million lambda req and 400k gb of compute time 
-   Integrated with the whole AWS suite of services 
-   Intergrated with many programming languages 
-   Easy montioring through AWS CloudWatch
-   Easy to get more resources per functions
-   Increasing RAM will also improve CPU and network 

AWS Lambda language support 
-   C#  (.Net core , PowerShell)
-   Nodejs 
-   PYTHON
-   JAVA (Java 8 + )
-   Golang
-   Ruby
-   Custom runtime API 

-   Lambda container Image
    +   The container image must implement the lambda runtime API
    +   ECS / Fargate is prefered for running arbitrary docker images 

Lambda Pricing : example 
-   Pay per CELLS :
    +   first 1million req free 
    +   0.2 per 1 million req there after 
-   Pay per DURATION 
    +   400k GB-seconds of compute time per month it FREE 
    +   == 400k seconds if function is 1GB RAM 
    +   == 3200000 seconds if function is 128MB RAM 
    +   After that $1.00 for 600k GB-seconds 
-   it is ususally VERY CHEAP to run AWS LAMBDA so it's very popular 

AWS Lambda limits to Know - PER REGION 
-   EXECUTION 
    +   Memory allocation : 128MB- 10 gb
    +   Maximum excution time : 15min
    +   Environment var (4kb)
    +   Disk capacity in the "function container " 512MB
    +   Concurrency executions : 1000
-   DEPLOYMENT 
    +   Lambda function deployment size (zip) : 50MB
    +   Lambda function uncompressed size (code + depends as not zip) : 250MB
    +   can use the/tmp directory to load other file at startup
    +   Environment var (4kb)

Lambda @ edge 
-   when u can use Lambda @ edge :
    DEPLOY LAMBDA FUNCTION ALONGSIDE YOUR CLOUDFRONT CDN
    +   build more responsive app
    +   you don't manage servers , Lambda is deployed globally
    +   Customize the CDN content 
    +   Pay only for want you use 

-   You can use to change CloudFront request and responses :
    +   After CloudFront receives a request from a viewer
    +   Before Cloudfront forwards the request to the origin
    +   After cloudFront receives the response from the origin
    +   Before CloudFront forwards the response to the viewer
-   You can also generate res to viewers without ever sending the req the origin

-   Use Cases : 
    +   Website Security and Privacy
    +   Dynamic web app at the EDGE 
    +   Search Engine Optimization (SEO)
    +   Interlligently Route Accross Origins and Data Centers 
    +   Bot Mitigation at the Edge
    +   Real-time Image Transformation 
    +   A/B Testing 
    +   User authen and Author 
    +   User Prioritization
    +   User Tracking and Analytics

Amazon DynamoDB
-   Fully managed , highly avaliable with replication accross multi AZ 
-   No SQL database - not a Relational database 
-   Scales to massive workloads, distributed database 
-   Million of request per seconds,trillions of row, 100s of TB of storage
-   Fast and consistent in perform 
-   Integrated with IAM for security, auther and administration
-   Enables event driven programming with Dynamodb Streams
-   Low cost and Auto scalling Capacibilities

DynamoDB - Basics
-   Dynamodb is made of TABLES 
-   Each table has a PRIMARY KEYS 
-   Each table can have an infinite number of items 
-   Each item has ATTRIBUTES 
-   maximum size of an item is 400kb
-   Data types support are : 
    +   Scalar Types : String, NUmber , null,Boolean, binary
    +   documents Types : List ,Map
    +   Set types : String set , Number Set , Binary Set

DynamoDB - Read / Write Capacity Modes 
-   Control how you manage your table's capacity
-   𝗣𝗿𝗼𝘃𝗶𝘀𝗶𝗼𝗻𝗲𝗱 𝗺𝗼𝗱𝗲 (default)
    +   You specify the number of reads/write per second 
    +   You need to plan capacity beforehand 
    +   Pay for provisioned Read Capacity Units(RCU)& Write Capacity Units
    +   Possibility to add 𝐴𝑢𝑡𝑜 𝑠𝑐𝑎𝑙𝑙𝑖𝑛𝑔 mode for RCU - WCU 
-   𝗢𝗡-𝗱𝗲𝗺𝗮𝗻𝗱 𝗺𝗼𝗱𝗲 
    +   Read/writes automatically scale up/down with your workloads 
    +   No capacity planning need 
    +   Pay for what you use , more exprensive
    +   Great for unpredictable workload

DynamoDB Accelerator (DAX)

-   Fully managed, highly avaliable, seamless in-memory cache for DynamoDB
-   𝗛𝗲𝗹𝗽 𝘀𝗼𝗹𝘃𝗲 𝗿𝗲𝗮𝗱 𝗰𝗼𝗻𝗴𝗲𝘀𝘁𝗶𝗼𝗻 𝗯𝘆 𝗰𝗮𝗰𝗵𝗶𝗻𝗴
-   Microseconds latency for cached data
-   Doesn't require application login modification
-   TTL cache for 5 min

DynamoDB Streams 
-   Ordered stream of item-level modifications (create/update/delete)
-   Stream records can be:
    +   Sent to Kinesis Data Streams 
    +   Read by AWS Lambda 
    +   Read by Kinesis Client Library applications 
-   Data retention for up to 24 hours 
-   Use cases :
    +   React to changes in real-time 
    +   Analytics
    +   Insert into derivative tables 
    +   Insert into ElasticSearch 
    +   Implement cross region replication

Dynamodb Global Tables 
-   Make a DynamoDB table accessible with LOW LATENCY in multiple regions
-   Active - Active replication
-   Applications can READ and Write to the table in any region
-   Must enable DynamoDB streams as a Pre requisite 

Dynamodb - Time to live (TTL )
-   Automatically delete item after an expiry timestamp
-   Use cases: reduce stored data by keeping only current items,
    adhere to regulatory obligations , ....

Dynamodb - Indexes 
-   Global Secondary Indexes (GSI) & Local secondary Indexes (LSI)
-   High level : Allow to QUERY on ATTRIBUTES other than Primary key

AWS API Gateway
-   AWS Lambda + API gateway : no infrastructure to manage
-   Support for the Websocket protocol
-   Handle API versioning (v1,v2,...)
-   Handle different environments( dev,test,prod )
-   Handle security(authen and author )
-   Create API keys , handle request throttling 
-   Swagger / Open API import to quickly define APIs 
-   Transform and validate requests and responses 
-   Generate API and SDK spcifications 
-   Cache API responses 

API Gateway - Intergrations High Level
-   LAMBDA FUNCTIONS 
    +   Invoke lambda functions 
    +   Easy way to expose REST API backend by AWS Lambda
-   HTTP 
    +   Expose HTTP endpoints in the backend 
    +   Example : internal HTTP API on premise, Application, Load Balancer ,...
    +   Why ? Add rate limiting, caching, user authentications, API keys, etc,...
-   AWS SERVICE 
    +   Expose any AWS API throught the API Gateway? 
    +   Example : Start an AWS Step function workflow , post a message to SQS
    +   Why ? Add authen, deploy publicly, rate control...

API Gateway - Enpoint Types 
-   EDGE OPTIMIZED (DEFAULT): For global clients 
    +   Request are routed through the CloudFront Edge locations
    +   The API Gateway still lives in ONLY ONE REGION 
-   REGIONAL 
    +   For clients within the same region 
    +   Cloud manually combine with CloudFront 
-   PRIVATE 
    +   Can be accessed from your VPC using interface VPC enpoint (ENI)
    +   Use a resource policy to define access 

API Gateway - Security 𝗜𝗔𝗠 𝗣𝗲𝗿𝗺𝗶𝘀𝘀𝗶𝗼𝗻
-   Create an IAM policy author and attach to USER / ROLE 
-   API Gateway verified IAM permisson to pass the calling applications 
-   Good to provice access within your own infrastructure
-   lervages "Sig v4" capability where crediental IAM are in header 

API Gateway - Security 𝗟𝗮𝗺𝗯𝗱𝗮 𝗮𝘂𝘁𝗵𝗼𝗿𝗶𝘇𝗲𝗿 (𝗙𝗼𝗿𝗺𝗲𝗹𝘆 𝗖𝘂𝘀𝘁𝗼𝗺 𝗔𝘂𝘁𝗵𝗼𝗿𝗶𝘇𝗲𝗿𝘀)
-   User AWS Lambda to validate token in header when being passed 
-   option to cache result of authentication
-   Helps to use OAuth / SAMPL / 3rd party type of authentication
-   Lambda must return an IAM policy for the user 

API Gateway - Security 𝗖𝗼𝗴𝗶𝗻𝘁𝗼 𝗨𝘀𝗲𝗿 𝗣𝗼𝗼𝗹𝘀 
-   coginto fully manage user lifecycle 
-   API gateway verified identity automatically from AWS conginto
-   no custom implementation required 
-   𝗖𝗼𝗴𝗶𝗻𝘁𝗼 𝗼𝗻𝗹𝘆 𝗵𝗲𝗹𝗽𝘀 𝘄𝗶𝘁𝗵 𝗮𝘂𝘁𝗵𝗲𝗻  ,𝗻𝗼𝘁 𝗮𝘂𝘁𝗵𝗲𝗿 

API Gateway - Security - Summary 
-   IAM
    +   Great for user / role already within AWS account 
    +   Handle auther/ authen 
    +   Leverage Sig v4 
-   Custom author 
    +   Great for 3rd party tokens 
    +   very flexible in term of what IAM policy is returned 
    +   Handle auther - authen 
    +   Pay per lambda invocation 
-   Coginto User Pool:
    +   You manage your own user pool
    +   no need to write any custom code 
    +   must implementation auther in the backend 

AWS Coginto
-   we want to give user identity so that they can interact with our app
-   COGINTO USER POOLS 
    +   Sign in functionality for app users 
    +   Intergrate with API gateway 
-   COGINTO IDENTITY POOLS (FEDERATED IDENTITY)
    +   Provide AWS Crediental user so thay can access AWS resources directly
    +   intergrate with coginto user pool as in identity provider
-   COGINTO SYNC 
    +   Sync data from device to coginto 
    +   decrepted and replaced by App sync 

AWS Coginto User Pools (CUP)
-   Create a serverless database of user for mobile app
-   Simple login : user name (email) and password combination 
-   Possibility to verified email / phone number and MFA 
-   Can enable FEDERATED Identities 
-   Send back a JWT TOKENS 
-   Can be intergrated with API Gateway for authentication

AWS Coginto - Ferderated Identity Pools 
-   Goal : 
    +   Provide direct access to AWS resources From Client Sides 
-   How : 
    +   login to FEDERATED identity provider - or remain anonymous 
    +   Get Temporary AWS crediental back from the Ferderated identity pools
    +   these crediental come with a pre-defined IAM policy stating
        their permissions 
Ex : Provide (temporary) access to write to s3 bucket using facebook login 

AWS Coginto SYNC 
-   decrepted  - USE APP SYNC now 
-   store preferences ,config , state of app 
-   cross defined Synchonization
-   offline capability
-   REQIRES FEDERATED IDENTITY POOLS 
-   Store data in dataset 
-   Up to 20 dataset to synchonise 

AWS SAM - Serverless application model
-   Framework for developing and deploying serverless applications 
-   All the config using YAML code 
    +   Lambda functions 
    +   DynamoDB tables 
    +   API gateway 
    +   Coginto userpools 
-   SAM can help you to run LAMBDA , API GATEWAY , Dynamodb locally
-   Sam can be used to CodeDeploy to deploy lambda functions 

